{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for Stage 3: Data Loading and Preprocessing\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define paths (adjust as needed)\n",
    "data_dir = 'ucf_crime_dataset'\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "val_dir = os.path.join(data_dir, 'val')\n",
    "train_annotations = 'train_annotations.txt'\n",
    "val_annotations = 'val_annotations.txt'\n",
    "\n",
    "# Load annotations\n",
    "train_df = pd.read_csv(train_annotations)  # Assumes CSV-like format with 'image_path', 'anomaly_type' columns\n",
    "val_df = pd.read_csv(val_annotations)\n",
    "\n",
    "# Define label parsing function\n",
    "def parse_labels(annotation):\n",
    "    anomaly_type = annotation['anomaly_type']  # Integer index (0â€“12 for anomalies, 13 for normal)\n",
    "    general_anomaly = 1 if anomaly_type < 13 else 0\n",
    "    violence = 1 if anomaly_type in [0, 3, 7] else 0  # Adjust indices for Assault, Fighting, Shooting\n",
    "    property_crime = 1 if anomaly_type in [2, 5, 6, 8] else 0  # Adjust indices for Burglary, Stealing, Shoplifting, Vandalism\n",
    "    return {'anomaly_type': anomaly_type, 'general_anomaly': general_anomaly, \n",
    "            'violence': violence, 'property_crime': property_crime}\n",
    "\n",
    "# Load and preprocess images\n",
    "def load_image(image_path, label):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [224, 224])\n",
    "    img = img / 255.0  # Normalize to [0,1]\n",
    "    return img, label\n",
    "\n",
    "# Create tf.data.Dataset\n",
    "def create_dataset(df, directory, batch_size=16, max_images=10000):\n",
    "    image_paths = [os.path.join(directory, path) for path in df['image_path']]\n",
    "    labels = [parse_labels(row) for _, row in df.iterrows()]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    dataset = dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.filter(lambda img, lbl: tf.reduce_all(tf.math.is_finite(img)))\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    dataset = dataset.take(max_images // batch_size)  # Limit to max_images\n",
    "    return dataset\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = create_dataset(train_df, train_dir, batch_size=16, max_images=10000)\n",
    "val_dataset = create_dataset(val_df, val_dir, batch_size=16, max_images=2000)\n",
    "\n",
    "# Verify dataset\n",
    "for img, labels in train_dataset.take(1):\n",
    "    print(f\"Image shape: {img.shape}, Label shapes: { {k: v.shape for k, v in labels.items()} }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5972608",
   "metadata": {},
   "source": [
    "# Stage 4: Single-Task Model for Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbd8f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for Stage 4: Single-Task Model for anomaly_type Baseline\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Build single-task model for anomaly_type\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "anomaly_type = layers.Dense(13, activation='softmax', name='anomaly_type')(x)\n",
    "model = Model(inputs=base_model.input, outputs=anomaly_type)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=5, verbose=1)\n",
    "\n",
    "# Evaluate AUC (One-vs-Rest)\n",
    "y_true, y_pred = [], []\n",
    "for img, labels in val_dataset:\n",
    "    preds = model.predict(img)\n",
    "    y_true.append(labels['anomaly_type'].numpy())\n",
    "    y_pred.append(preds)\n",
    "y_true = np.concatenate(y_true)\n",
    "y_pred = np.concatenate(y_pred)\n",
    "auc = roc_auc_score(y_true, y_pred, multi_class='ovr')\n",
    "print(f\"Baseline anomaly_type AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a369a6",
   "metadata": {},
   "source": [
    "# Stage 5: Multi-Task Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6097efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for Stage 5: Multi-Task Model Design\n",
    "# Reusing imports from Stage 4\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(512, activation='relu', name='dense_512')(x)\n",
    "# Task-specific heads\n",
    "anomaly_type = layers.Dense(13, activation='softmax', name='anomaly_type')(x)\n",
    "general_anomaly = layers.Dense(1, activation='sigmoid', name='general_anomaly')(x)\n",
    "violence = layers.Dense(1, activation='sigmoid', name='violence')(x)\n",
    "property_crime = layers.Dense(1, activation='sigmoid', name='property_crime')(x)\n",
    "model = Model(inputs=base_model.input, outputs=[anomaly_type, general_anomaly, violence, property_crime])\n",
    "\n",
    "# Compile with weighted losses\n",
    "losses = {\n",
    "    'anomaly_type': 'sparse_categorical_crossentropy',\n",
    "    'general_anomaly': 'binary_crossentropy',\n",
    "    'violence': 'binary_crossentropy',\n",
    "    'property_crime': 'binary_crossentropy'\n",
    "}\n",
    "loss_weights = {'anomaly_type': 1.0, 'general_anomaly': 0.5, 'violence': 0.5, 'property_crime': 0.5}\n",
    "model.compile(optimizer='adam', loss=losses, loss_weights=loss_weights, metrics=['accuracy'])\n",
    "\n",
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e67744",
   "metadata": {},
   "source": [
    "# Stage 6: Multi-Task Training and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654bbdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for Stage 6: Multi-Task Training and Analysis\n",
    "# Compute gradient similarity\n",
    "def compute_gradient_similarity(model, data):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(data[0], training=True)\n",
    "        anomaly_type_loss = tf.reduce_mean(model.losses[0])  # anomaly_type loss\n",
    "        subtask_losses = [tf.reduce_mean(loss) for loss in model.losses[1:]]\n",
    "    gradients = tape.gradient([anomaly_type_loss] + subtask_losses, model.trainable_variables)\n",
    "    anomaly_type_grads = gradients[0]\n",
    "    subtask_grads = gradients[1:]\n",
    "    similarities = [tf.reduce_mean(tf.keras.metrics.cosine_similarity(anomaly_type_grads, g)) for g in subtask_grads]\n",
    "    return similarities\n",
    "\n",
    "# Train model\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=10, verbose=1)\n",
    "\n",
    "# Analyze gradient similarity\n",
    "for img, labels in val_dataset.take(1):\n",
    "    similarities = compute_gradient_similarity(model, (img, labels))\n",
    "    print(f\"Gradient similarities (anomaly_type vs. sub-tasks): {similarities}\")\n",
    "\n",
    "# Store loss history for Stage 9\n",
    "loss_history = {key: history.history[f'{key}_loss'] for key in ['anomaly_type', 'general_anomaly', 'violence', 'property_crime']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dd0bf7",
   "metadata": {},
   "source": [
    "# Stage 7: Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ddcffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for Stage 7: Evaluation Metrics\n",
    "# Evaluate OvR AUC for anomaly_type and binary AUC for sub-tasks\n",
    "y_true, y_pred = {'anomaly_type': [], 'general_anomaly': [], 'violence': [], 'property_crime': []}, {'anomaly_type': [], 'general_anomaly': [], 'violence': [], 'property_crime': []}\n",
    "for img, labels in val_dataset:\n",
    "    preds = model.predict(img)\n",
    "    y_true['anomaly_type'].append(labels['anomaly_type'].numpy())\n",
    "    y_true['general_anomaly'].append(labels['general_anomaly'].numpy())\n",
    "    y_true['violence'].append(labels['violence'].numpy())\n",
    "    y_true['property_crime'].append(labels['property_crime'].numpy())\n",
    "    y_pred['anomaly_type'].append(preds[0])\n",
    "    y_pred['general_anomaly'].append(preds[1])\n",
    "    y_pred['violence'].append(preds[2])\n",
    "    y_pred['property_crime'].append(preds[3])\n",
    "\n",
    "# Compute AUC\n",
    "for task in y_true:\n",
    "    y_true[task] = np.concatenate(y_true[task])\n",
    "    y_pred[task] = np.concatenate(y_pred[task])\n",
    "    auc = roc_auc_score(y_true[task], y_pred[task], multi_class='ovr' if task == 'anomaly_type' else None)\n",
    "    print(f\"{task} AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc543905",
   "metadata": {},
   "source": [
    "# Stage 8: Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32306d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for Stage 8: Ablation Study\n",
    "# Define task combinations\n",
    "combinations = [\n",
    "    ['anomaly_type'],\n",
    "    ['anomaly_type', 'general_anomaly'],\n",
    "    ['anomaly_type', 'violence', 'property_crime'],\n",
    "    ['anomaly_type', 'general_anomaly', 'violence', 'property_crime']\n",
    "]\n",
    "\n",
    "# Run ablation study\n",
    "auc_results = {}\n",
    "for combo in combinations:\n",
    "    # Build model for combination\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    outputs = []\n",
    "    if 'anomaly_type' in combo:\n",
    "        outputs.append(layers.Dense(13, activation='softmax', name='anomaly_type')(x))\n",
    "    if 'general_anomaly' in combo:\n",
    "        outputs.append(layers.Dense(1, activation='sigmoid', name='general_anomaly')(x))\n",
    "    if 'violence' in combo:\n",
    "        outputs.append(layers.Dense(1, activation='sigmoid', name='violence')(x))\n",
    "    if 'property_crime' in combo:\n",
    "        outputs.append(layers.Dense(1, activation='sigmoid', name='property_crime')(x))\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    \n",
    "    # Compile\n",
    "    losses = {out.name: 'sparse_categorical_crossentropy' if out.name == 'anomaly_type' else 'binary_crossentropy' for out in outputs}\n",
    "    loss_weights = {out.name: 1.0 if out.name == 'anomaly_type' else 0.5 for out in outputs}\n",
    "    model.compile(optimizer='adam', loss=losses, loss_weights=loss_weights, metrics=['accuracy'])\n",
    "    \n",
    "    # Train\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=5, verbose=0)\n",
    "    \n",
    "    # Evaluate anomaly_type AUC\n",
    "    y_true, y_pred = [], []\n",
    "    for img, labels in val_dataset:\n",
    "        preds = model.predict(img)\n",
    "        y_true.append(labels['anomaly_type'].numpy())\n",
    "        y_pred.append(preds[0] if len(combo) == 1 else preds[combo.index('anomaly_type')])\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred, multi_class='ovr')\n",
    "    auc_results[str(combo)] = auc\n",
    "    print(f\"AUC for {combo}: {auc:.4f}\")\n",
    "\n",
    "print(\"Ablation study results:\", auc_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd894087",
   "metadata": {},
   "source": [
    "# Stage 9: Task Relationship Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725a4998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for Stage 9: Task Relationship Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# 1. Gradient Similarity Heatmap\n",
    "tasks = ['anomaly_type', 'general_anomaly', 'violence', 'property_crime']\n",
    "sim_matrix = np.zeros((len(tasks), len(tasks)))\n",
    "for img, labels in val_dataset.take(1):\n",
    "    similarities = compute_gradient_similarity(model, (img, labels))  # From Stage 6\n",
    "    sim_matrix[0, 1:] = similarities\n",
    "sns.heatmap(sim_matrix, xticklabels=tasks, yticklabels=tasks, annot=True, cmap='Blues')\n",
    "plt.title('Gradient Similarity Between Tasks')\n",
    "plt.savefig('gradient_similarity.png')\n",
    "plt.show()\n",
    "\n",
    "# 2. Loss Correlation Matrix\n",
    "loss_df = pd.DataFrame(loss_history)  # From Stage 6\n",
    "corr_matrix = loss_df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Task Loss Correlation')\n",
    "plt.savefig('loss_correlation.png')\n",
    "plt.show()\n",
    "\n",
    "# 3. t-SNE Feature Visualization\n",
    "activation_model = Model(inputs=model.input, outputs=model.get_layer('dense_512').output)\n",
    "features, y_true = [], []\n",
    "for img, labels in val_dataset:\n",
    "    features.append(activation_model.predict(img))\n",
    "    y_true.append(labels['anomaly_type'].numpy())\n",
    "features = np.concatenate(features)\n",
    "y_true = np.concatenate(y_true)\n",
    "tsne = TSNE(n_components=2, random_state=42).fit_transform(features)\n",
    "plt.scatter(tsne[:, 0], tsne[:, 1], c=y_true, cmap='viridis')\n",
    "plt.title('t-SNE of Shared Layer Features by Anomaly Type')\n",
    "plt.savefig('tsne_features.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b875a03a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
