{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7abb53a",
   "metadata": {},
   "source": [
    "# UCF-Crime Anomaly Detection with Multi-Task Learning\n",
    "\n",
    "This Jupyter Notebook implements a Multi-Task Learning (MTL) pipeline for anomaly detection using an image-based UCF-Crime dataset. The pipeline processes `.png` images, trains a model with four tasks (general anomaly detection, violence detection, property crime detection, anomaly type classification), analyzes task relationships, and conducts an ablation study. The dataset is assumed to be at `/home/user/ucf_crime_dataset` with generated annotation files.\n",
    "\n",
    "## Setup Instructions\n",
    "1. Install dependencies: `pip install tensorflow opencv-python numpy pandas scikit-learn scipy`\n",
    "2. Download the Kaggle dataset: https://www.kaggle.com/datasets/mission-ai/crimeucfdataset\n",
    "3. Extract `.png` images or use `extract_frames.py` to convert videos to images.\n",
    "4. Update paths in cells as needed (e.g., `data_dir`).\n",
    "5. Run cells sequentially, inspecting outputs for debugging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d12656",
   "metadata": {},
   "source": [
    "# Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from scipy.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f31cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "DATA_DIR = '/home/user/ucf_crime_dataset'  # Update to your dataset path\n",
    "TRAIN_ANNOTATION_FILE = 'train_annotations.txt'\n",
    "TEST_ANNOTATION_FILE = 'test_annotations.txt'\n",
    "IMAGE_SIZE = (224, 224)  # Image size for ResNet50\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "NUM_ANOMALY_TYPES = 14  # 13 anomaly classes + Normal\n",
    "\n",
    "# Verify dataset directory\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    raise ValueError(f\"Dataset directory {DATA_DIR} does not exist. Update DATA_DIR.\")\n",
    "\n",
    "print(\"Setup complete. Dataset directory:\", DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9848ac88",
   "metadata": {},
   "source": [
    "# Stage 2: Annotation Generation\n",
    "Generate annotation files (train_annotations.txt, test_annotations.txt) for the image-based dataset, mapping .png images to class labels based on folder structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12395d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def generate_annotation_file(dataset_root, split, output_file):\n",
    "    \"\"\"\n",
    "    Generate an annotation file for a given dataset split (train or test) with .png images.\n",
    "    \"\"\"\n",
    "    split_dir = os.path.join(dataset_root, split)\n",
    "    if not os.path.exists(split_dir):\n",
    "        raise ValueError(f\"Directory {split_dir} does not exist.\")\n",
    "    \n",
    "    annotations = []\n",
    "    for class_name in os.listdir(split_dir):\n",
    "        class_dir = os.path.join(split_dir, class_name)\n",
    "        if not os.path.isdir(class_dir):\n",
    "            continue\n",
    "        \n",
    "        image_files = glob.glob(os.path.join(class_dir, \"*.png\"))\n",
    "        for image_path in image_files:\n",
    "            relative_path = os.path.relpath(image_path, dataset_root)\n",
    "            annotation = f\"{relative_path} {class_name}\"\n",
    "            annotations.append(annotation)\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        for annotation in annotations:\n",
    "            f.write(annotation + '\\n')\n",
    "    \n",
    "    print(f\"Generated {output_file} with {len(annotations)} entries.\")\n",
    "\n",
    "# Generate annotations for train and test splits\n",
    "generate_annotation_file(DATA_DIR, 'train', TRAIN_ANNOTATION_FILE)\n",
    "generate_annotation_file(DATA_DIR, 'test', TEST_ANNOTATION_FILE)\n",
    "\n",
    "# Verify annotation files\n",
    "if os.path.exists(TRAIN_ANNOTATION_FILE) and os.path.exists(TEST_ANNOTATION_FILE):\n",
    "    print(\"Annotation files created successfully.\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Annotation files not found. Check generation process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e39573c",
   "metadata": {},
   "source": [
    "# Stage 3: Data Loading\n",
    "Load .png images and labels from the annotation files using load_ucf_crime_data. Outputs the number of loaded images and label shapes for debugging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1172c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ucf_crime_data(data_dir, annotation_file, image_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Load UCF-Crime dataset images and annotations from the generated annotation file.\n",
    "    \"\"\"\n",
    "    annotations = pd.read_csv(annotation_file, sep=' ', header=None, names=['image', 'label'])\n",
    "    \n",
    "    images = []\n",
    "    labels = {\n",
    "        'general_anomaly': [],\n",
    "        'violence': [],\n",
    "        'property_crime': [],\n",
    "        'anomaly_type': []\n",
    "    }\n",
    "    \n",
    "    anomaly_classes = ['Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Explosion',\n",
    "                       'Fighting', 'Robbery', 'Shooting', 'Stealing', 'Shoplifting',\n",
    "                       'Vandalism', 'RoadAccident']\n",
    "    violent_classes = ['Assault', 'Fighting', 'Shooting']\n",
    "    property_classes = ['Burglary', 'Stealing', 'Shoplifting', 'Vandalism']\n",
    "    \n",
    "    for _, row in annotations.iterrows():\n",
    "        image_path = os.path.join(data_dir, row['image'])\n",
    "        label = row['label']\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(f\"Failed to load image: {image_path}\")\n",
    "            continue\n",
    "        \n",
    "        image = cv2.resize(image, image_size)\n",
    "        image = image / 255.0\n",
    "        images.append(image)\n",
    "        \n",
    "        is_anomaly = 1 if label in anomaly_classes else 0\n",
    "        is_violent = 1 if label in violent_classes else 0\n",
    "        is_property = 1 if label in property_classes else 0\n",
    "        anomaly_type = anomaly_classes.index(label) if label in anomaly_classes else len(anomaly_classes)\n",
    "        \n",
    "        labels['general_anomaly'].append(is_anomaly)\n",
    "        labels['violence'].append(is_violent)\n",
    "        labels['property_crime'].append(is_property)\n",
    "        labels['anomaly_type'].append(anomaly_type)\n",
    "    \n",
    "    images = np.array(images)\n",
    "    for task in labels:\n",
    "        labels[task] = np.array(labels[task])\n",
    "    \n",
    "    print(f\"Loaded {len(images)} images from {annotation_file}\")\n",
    "    print(\"Label shapes:\", {task: len(labels[task]) for task in labels})\n",
    "    return images, labels\n",
    "\n",
    "# Load training and validation data\n",
    "X_train, y_train = load_ucf_crime_data(DATA_DIR, TRAIN_ANNOTATION_FILE, IMAGE_SIZE)\n",
    "X_val, y_val = load_ucf_crime_data(DATA_DIR, TEST_ANNOTATION_FILE, IMAGE_SIZE)\n",
    "\n",
    "# Verify data shapes\n",
    "print(\"Training images shape:\", X_train.shape)\n",
    "print(\"Validation images shape:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3942d0ec",
   "metadata": {},
   "source": [
    "# Stage 4: Model Definition\n",
    "Define the MTL model with a ResNet50 backbone and four task-specific heads using build_mtl_model. Display the model summary for inspection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c5d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mtl_model(input_shape=(224, 224, 3), num_anomaly_types=14):\n",
    "    \"\"\"\n",
    "    Build MTL model with shared ResNet50 backbone and task-specific heads for images.\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    x = base_model(inputs)\n",
    "    pooled = layers.GlobalAveragePooling2D()(x)\n",
    "    shared_dense = layers.Dense(512, activation='relu')(pooled)\n",
    "    \n",
    "    anomaly_output = layers.Dense(1, activation='sigmoid', name='general_anomaly')(shared_dense)\n",
    "    violence_output = layers.Dense(1, activation='sigmoid', name='violence')(shared_dense)\n",
    "    property_output = layers.Dense(1, activation='sigmoid', name='property_crime')(shared_dense)\n",
    "    type_output = layers.Dense(num_anomaly_types, activation='softmax', name='anomaly_type')(shared_dense)\n",
    "    \n",
    "    model = models.Model(inputs, [\n",
    "        anomaly_output, violence_output, property_output, type_output\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model and display summary\n",
    "model = build_mtl_model(input_shape=(224, 224, 3), num_anomaly_types=NUM_ANOMALY_TYPES)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4acec21",
   "metadata": {},
   "source": [
    "# Stage 5: Task Relationship Functions\n",
    "Define functions to analyze task relationships: compute_gradient_alignment (cosine similarity of gradients) and compute_loss_correlation (Pearson correlation of losses).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eda9c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_alignment(model, data, labels, task_names):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between gradients of tasks.\n",
    "    \"\"\"\n",
    "    gradients = {}\n",
    "    for task in task_names:\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(data)\n",
    "            task_idx = task_names.index(task)\n",
    "            loss = tf.keras.losses.binary_crossentropy(labels[task], predictions[task_idx])\n",
    "            if task == 'anomaly_type':\n",
    "                loss = tf.keras.losses.sparse_categorical_crossentropy(labels[task], predictions[task_idx])\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        gradients[task] = grads\n",
    "    \n",
    "    similarities = {}\n",
    "    for task1 in task_names:\n",
    "        for task2 in task_names:\n",
    "            if task1 >= task2:\n",
    "                continue\n",
    "            g1 = tf.concat([tf.reshape(g, [-1]) for g in gradients[task1] if g is not None], axis=0)\n",
    "            g2 = tf.concat([tf.reshape(g, [-1]) for g in gradients[task2] if g is not None], axis=0)\n",
    "            cos_sim = tf.reduce_sum(g1 * g2) / (tf.norm(g1) * tf.norm(g2))\n",
    "            similarities[f'{task1}_vs_{task2}'] = cos_sim.numpy()\n",
    "    \n",
    "    print(\"Computed gradient similarities:\", similarities)\n",
    "    return similarities\n",
    "\n",
    "def compute_loss_correlation(losses_dict, task_names):\n",
    "    \"\"\"\n",
    "    Compute Pearson correlation between task losses.\n",
    "    \"\"\"\n",
    "    correlations = {}\n",
    "    for task1 in task_names:\n",
    "        for task2 in task_names:\n",
    "            if task1 >= task2:\n",
    "                continue\n",
    "            corr, _ = pearsonr(losses_dict[task1], losses_dict[task2])\n",
    "            correlations[f'{task1}_vs_{task2}'] = corr\n",
    "    print(\"Computed loss correlations:\", correlations)\n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47639eb4",
   "metadata": {},
   "source": [
    "# Stage 6: Training Function\n",
    "Define train_mtl_model to train the MTL model, compute task relationships, and return training history. Monitor training progress via printed outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47616fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mtl_model(model, X_train, y_train, X_val, y_val, epochs=10, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train MTL model and collect losses for correlation analysis.\n",
    "    \"\"\"\n",
    "    task_names = ['general_anomaly', 'violence', 'property_crime', 'anomaly_type']\n",
    "    losses_dict = {task: [] for task in task_names}\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss={\n",
    "            'general_anomaly': 'binary_crossentropy',\n",
    "            'violence': 'binary_crossentropy',\n",
    "            'property_crime': 'binary_crossentropy',\n",
    "            'anomaly_type': 'sparse_categorical_crossentropy'\n",
    "        },\n",
    "        loss_weights={\n",
    "            'general_anomaly': 1.0,\n",
    "            'violence': 0.5,\n",
    "            'property_crime': 0.5,\n",
    "            'anomaly_type': 1.0\n",
    "        },\n",
    "        metrics={\n",
    "            'general_anomaly': ['accuracy', tf.keras.metrics.AUC(name='auc')],\n",
    "            'violence': ['accuracy'],\n",
    "            'property_crime': ['accuracy'],\n",
    "            'anomaly_type': ['accuracy']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        [y_train[task] for task in task_names],\n",
    "        validation_data=(X_val, [y_val[task] for task in task_names]),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    for task in task_names:\n",
    "        losses_dict[task] = history.history[f'{task}_loss']\n",
    "    \n",
    "    grad_similarities = compute_gradient_alignment(model, X_val[:batch_size], y_val, task_names)\n",
    "    loss_correlations = compute_loss_correlation(losses_dict, task_names)\n",
    "    \n",
    "    return history, grad_similarities, loss_correlations\n",
    "\n",
    "# Train model (uncomment to run after verifying previous stages)\n",
    "# history, grad_similarities, loss_correlations = train_mtl_model(model, X_train, y_train, X_val, y_val, epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2175a8b6",
   "metadata": {},
   "source": [
    "# Stage 7: Ablation Study\n",
    "Define ablation_study to evaluate the model with subsets of tasks, reporting AUC for general anomaly detection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f8379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablation_study(X_train, y_train, X_val, y_val, tasks_to_include, input_shape=(224, 224, 3), num_anomaly_types=14):\n",
    "    \"\"\"\n",
    "    Train model with a subset of tasks for ablation study.\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    x = base_model(inputs)\n",
    "    pooled = layers.GlobalAveragePooling2D()(x)\n",
    "    shared_dense = layers.Dense(512, activation='relu')(pooled)\n",
    "    \n",
    "    outputs = []\n",
    "    loss_dict = {}\n",
    "    metrics_dict = {}\n",
    "    \n",
    "    for task in tasks_to_include:\n",
    "        if task in ['general_anomaly', 'violence', 'property_crime']:\n",
    "            output = layers.Dense(1, activation='sigmoid', name=task)(shared_dense)\n",
    "            loss_dict[task] = 'binary_crossentropy'\n",
    "            metrics_dict[task] = ['accuracy']\n",
    "        elif task == 'anomaly_type':\n",
    "            output = layers.Dense(num_anomaly_types, activation='softmax', name=task)(shared_dense)\n",
    "            loss_dict[task] = 'sparse_categorical_crossentropy'\n",
    "            metrics_dict[task] = ['accuracy']\n",
    "        outputs.append(output)\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=loss_dict,\n",
    "        metrics=metrics_dict\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        [y_train[task] for task in tasks_to_include],\n",
    "        validation_data=(X_val, [y_val[task] for task in tasks_to_include]),\n",
    "        epochs=5,\n",
    "        batch_size=32,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    val_preds = model.predict(X_val)\n",
    "    general_idx = tasks_to_include.index('general_anomaly') if 'general_anomaly' in tasks_to_include else None\n",
    "    if general_idx is not None:\n",
    "        auc = roc_auc_score(y_val['general_anomaly'], val_preds[general_idx])\n",
    "        print(f\"AUC for tasks {tasks_to_include}: {auc:.4f}\")\n",
    "        return auc\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112054fc",
   "metadata": {},
   "source": [
    "# Stage 8: Execution and Results\n",
    "Run the pipeline, train the model, compute task relationships, and perform the ablation study. Display results for gradient alignment, loss correlation, and AUC scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afa51d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history, grad_similarities, loss_correlations = train_mtl_model(\n",
    "    model, X_train, y_train, X_val, y_val, epochs=EPOCHS, batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Print task relationship results\n",
    "print(\"\\nGradient Alignment (Cosine Similarity):\")\n",
    "for pair, sim in grad_similarities.items():\n",
    "    print(f\"{pair}: {sim:.4f}\")\n",
    "\n",
    "print(\"\\nLoss Correlation (Pearson):\")\n",
    "for pair, corr in loss_correlations.items():\n",
    "    print(f\"{pair}: {corr:.4f}\")\n",
    "\n",
    "# Perform ablation study\n",
    "task_combinations = [\n",
    "    ['general_anomaly'],\n",
    "    ['general_anomaly', 'violence', 'property_crime'],\n",
    "    ['general_anomaly', 'anomaly_type'],\n",
    "    ['general_anomaly', 'violence', 'property_crime', 'anomaly_type']\n",
    "]\n",
    "\n",
    "print(\"\\nAblation Study Results (AUC for General Anomaly Detection):\")\n",
    "for tasks in task_combinations:\n",
    "    auc = ablation_study(X_train, y_train, X_val, y_val, tasks)\n",
    "    if auc is not None:\n",
    "        print(f\"Tasks: {tasks}, AUC: {auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
