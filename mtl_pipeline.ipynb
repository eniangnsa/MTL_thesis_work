{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7abb53a",
   "metadata": {},
   "source": [
    "# UCF-Crime Anomaly Detection with Multi-Task Learning\n",
    "\n",
    "This Jupyter Notebook implements a Multi-Task Learning (MTL) pipeline for anomaly detection using an image-based UCF-Crime dataset. The pipeline processes `.png` images, trains a model with four tasks (general anomaly detection, violence detection, property crime detection, anomaly type classification), analyzes task relationships, and conducts an ablation study. The dataset is assumed to be at `/home/user/ucf_crime_dataset` with generated annotation files.\n",
    "\n",
    "## Setup Instructions\n",
    "1. Install dependencies: `pip install tensorflow opencv-python numpy pandas scikit-learn scipy`\n",
    "2. Download the Kaggle dataset: https://www.kaggle.com/datasets/mission-ai/crimeucfdataset\n",
    "3. Extract `.png` images or use `extract_frames.py` to convert videos to images.\n",
    "4. Update paths in cells as needed (e.g., `data_dir`).\n",
    "5. Run cells sequentially, inspecting outputs for debugging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d12656",
   "metadata": {},
   "source": [
    "# Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef5f007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from scipy.stats import pearsonr\n",
    "import csv\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7f31cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train subfolders:  ['Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Explosion', 'Fighting', 'NormalVideos', 'RoadAccidents', 'Robbery', 'Shooting', 'Shoplifting', 'Stealing', 'Vandalism']\n",
      "Test subfolders:  ['Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Explosion', 'Fighting', 'NormalVideos', 'RoadAccidents', 'Robbery', 'Shooting', 'Shoplifting', 'Stealing', 'Vandalism']\n",
      "Setup complete. Dataset directory: D:/Users/eniang.eniang/Desktop/Multi-task learning/data/\n"
     ]
    }
   ],
   "source": [
    "# Define constants\n",
    "DATA_DIR = 'D:/Users/eniang.eniang/Desktop/Multi-task learning/data/'  # Update to your dataset path\n",
    "TRAIN_ANNOTATION_FILE = 'train_annotations.txt'\n",
    "TEST_ANNOTATION_FILE = 'test_annotations.txt'\n",
    "IMAGE_SIZE = (224, 224)  # Image size for ResNet50\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 1\n",
    "NUM_ANOMALY_TYPES = 14  # 13 anomaly classes + Normal\n",
    "\n",
    "# Verify dataset directory\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    raise ValueError(f\"Dataset directory {DATA_DIR} does not exist. Update DATA_DIR.\")\n",
    "\n",
    "# check train and test directories\n",
    "train_dir = os.path.join(DATA_DIR, \"Train\")\n",
    "test_dir = os.path.join(DATA_DIR, \"Test\")\n",
    "\n",
    "print(\"Train subfolders: \", os.listdir(train_dir) if os.path.exists(train_dir) else \"Not found\")\n",
    "print(\"Test subfolders: \", os.listdir(test_dir) if os.path.exists(test_dir) else \"Not found\")\n",
    "\n",
    "print(\"Setup complete. Dataset directory:\", DATA_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9848ac88",
   "metadata": {},
   "source": [
    "# Stage 2: Annotation Generation\n",
    "Generate annotation files (train_annotations.txt, test_annotations.txt) for the image-based dataset, mapping .png images to class labels based on folder structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12395d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training annotations..........\n",
      "Found 14 class subfolders in D:/Users/eniang.eniang/Desktop/Multi-task learning/data/Train: ['Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Explosion', 'Fighting', 'NormalVideos', 'RoadAccidents', 'Robbery', 'Shooting', 'Shoplifting', 'Stealing', 'Vandalism']\n",
      "Generated train_annotations.txt with 2532690 entries\n",
      "Images per class in Train: {'Abuse': 38152, 'Arrest': 52794, 'Arson': 48842, 'Assault': 20720, 'Burglary': 79008, 'Explosion': 37506, 'Fighting': 49368, 'NormalVideos': 1895536, 'RoadAccidents': 46972, 'Robbery': 82986, 'Shooting': 14280, 'Shoplifting': 49670, 'Stealing': 89604, 'Vandalism': 27252}\n",
      "Sample annotations (first 5): ['\"Train\\\\Abuse\\\\Abuse001_x264_0.png\" Abuse', '\"Train\\\\Abuse\\\\Abuse001_x264_10.png\" Abuse', '\"Train\\\\Abuse\\\\Abuse001_x264_100.png\" Abuse', '\"Train\\\\Abuse\\\\Abuse001_x264_1000.png\" Abuse', '\"Train\\\\Abuse\\\\Abuse001_x264_1010.png\" Abuse']\n",
      "Generating test annotations..............\n",
      "Found 14 class subfolders in D:/Users/eniang.eniang/Desktop/Multi-task learning/data/Test: ['Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Explosion', 'Fighting', 'NormalVideos', 'RoadAccidents', 'Robbery', 'Shooting', 'Shoplifting', 'Stealing', 'Vandalism']\n",
      "Generated test_annotations.txt with 222616 entries\n",
      "Images per class in Test: {'Abuse': 594, 'Arrest': 6730, 'Arson': 5586, 'Assault': 5314, 'Burglary': 15314, 'Explosion': 13020, 'Fighting': 2462, 'NormalVideos': 129904, 'RoadAccidents': 5326, 'Robbery': 1670, 'Shooting': 15260, 'Shoplifting': 15246, 'Stealing': 3968, 'Vandalism': 2222}\n",
      "Sample annotations (first 5): ['\"Test\\\\Abuse\\\\Abuse028_x264_0.png\" Abuse', '\"Test\\\\Abuse\\\\Abuse028_x264_10.png\" Abuse', '\"Test\\\\Abuse\\\\Abuse028_x264_100.png\" Abuse', '\"Test\\\\Abuse\\\\Abuse028_x264_1000.png\" Abuse', '\"Test\\\\Abuse\\\\Abuse028_x264_1010.png\" Abuse']\n",
      "\n",
      "Annotation files created successfully.\n",
      "First 5 lines of the train_annotation.txt:\n",
      "['\"Train\\\\Abuse\\\\Abuse001_x264_0.png\" Abuse\\n', '\"Train\\\\Abuse\\\\Abuse001_x264_10.png\" Abuse\\n', '\"Train\\\\Abuse\\\\Abuse001_x264_100.png\" Abuse\\n', '\"Train\\\\Abuse\\\\Abuse001_x264_1000.png\" Abuse\\n', '\"Train\\\\Abuse\\\\Abuse001_x264_1010.png\" Abuse\\n']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "def generate_annotation_file(dataset_root, split, output_file):\n",
    "    \"\"\"\n",
    "    Generate an annotation file for a given dataset split (train or test) with .png images.\n",
    "    \"\"\"\n",
    "    split_dir = os.path.join(dataset_root, split)\n",
    "    if not os.path.exists(split_dir):\n",
    "        raise ValueError(f\"Directory {split_dir} does not exist.\")\n",
    "    \n",
    "    annotations = []\n",
    "    class_counts = {}\n",
    "\n",
    "    # List the class subfolders\n",
    "    class_names = [name for name in os.listdir(split_dir) if os.path.isdir(os.path.join(split_dir, name))]\n",
    "    print(f\"Found {len(class_names)} class subfolders in {split_dir}: {class_names}\")\n",
    "\n",
    "    for class_name in class_names:\n",
    "        class_dir = os.path.join(split_dir, class_name)\n",
    "\n",
    "        # Find the images with multiple extensions\n",
    "        image_files = []\n",
    "        for ext in [\"*.jpg\", \"*.PNG\", \"*.png\", \"*.JPG\"]:\n",
    "            image_files.extend(glob.glob(os.path.join(class_dir, ext)))\n",
    "\n",
    "        class_counts[class_name] = len(image_files)\n",
    "\n",
    "        for image_path in image_files:\n",
    "            # construct relative path\n",
    "            relative_path = os.path.join(split, class_name, os.path.basename(image_path))\n",
    "            # Quote path to handle spaces\n",
    "            annotation = f'\"{relative_path}\" {class_name}'\n",
    "            annotations.append(annotation)\n",
    "\n",
    "    # Write the annotations\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for annotation in annotations:\n",
    "            f.write(annotation + '\\n')\n",
    "    \n",
    "    # debugging output\n",
    "    print(f\"Generated {output_file} with {len(annotations)} entries\")\n",
    "    print(f\"Images per class in {split}:\", class_counts)\n",
    "    if annotations:\n",
    "        print(\"Sample annotations (first 5):\", annotations[:5])\n",
    "    if not annotations:\n",
    "        print(\"WARNING!!! no images found\")\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "\n",
    "# Generate Annotations\n",
    "print(\"Generating training annotations..........\")\n",
    "train_class_counts = generate_annotation_file(DATA_DIR, \"Train\", TRAIN_ANNOTATION_FILE)\n",
    "print(\"Generating test annotations..............\")\n",
    "test_class_counts = generate_annotation_file(DATA_DIR, \"Test\", TEST_ANNOTATION_FILE)\n",
    "\n",
    "# verify annotation files\n",
    "if os.path.exists(TRAIN_ANNOTATION_FILE) and os.path.exists(TEST_ANNOTATION_FILE):\n",
    "    print(\"\\nAnnotation files created successfully.\")\n",
    "    print(\"First 5 lines of the train_annotation.txt:\")\n",
    "    with open(TRAIN_ANNOTATION_FILE, 'r') as f:\n",
    "        print(f.readlines()[:5])\n",
    "\n",
    "else:\n",
    "    raise FileNotFoundError(\"Annotation file not found. Check generation process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e39573c",
   "metadata": {},
   "source": [
    "# Stage 3: Data Loading\n",
    "Load .png images and labels from the annotation files using load_ucf_crime_data. Outputs the number of loaded images and label shapes for debugging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1172c912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Loaded annotation file: train_annotations.txt\n",
      "Total annotations: 2532690\n",
      "Subsampled to 10000 images\n",
      "Valid images: 10000\n",
      "Dataset spec before map: (TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))\n",
      "Dataset spec after map: (TensorSpec(shape=(224, 224, 3), dtype=tf.float32, name=None), {'general_anomaly': TensorSpec(shape=(1,), dtype=tf.int32, name=None), 'violence': TensorSpec(shape=(1,), dtype=tf.int32, name=None), 'property_crime': TensorSpec(shape=(1,), dtype=tf.int32, name=None), 'anomaly_type': TensorSpec(shape=(), dtype=tf.int32, name=None)})\n",
      "Images before filtering: 10000\n",
      "Dataset spec after filter: (TensorSpec(shape=(224, 224, 3), dtype=tf.float32, name=None), {'general_anomaly': TensorSpec(shape=(1,), dtype=tf.int32, name=None), 'violence': TensorSpec(shape=(1,), dtype=tf.int32, name=None), 'property_crime': TensorSpec(shape=(1,), dtype=tf.int32, name=None), 'anomaly_type': TensorSpec(shape=(), dtype=tf.int32, name=None)})\n",
      "Images after filtering: 9997\n",
      "Batch image shape: (16, 224, 224, 3)\n",
      "Batch label shapes: { general_anomaly: (16, 1), violence: (16, 1), property_crime: (16, 1), anomaly_type: (16,) }\n",
      "Sample anomaly_type values: [ 6 13 13 13 13]\n",
      "Expected steps per epoch: 625 (with batch_size=16)\n",
      "Returning tf.data.Dataset for train_annotations.txt\n",
      "Time taken: 13.56 seconds\n",
      "\n",
      "Loading validation data...\n",
      "Loaded annotation file: test_annotations.txt\n",
      "Total annotations: 222616\n",
      "Subsampled to 2000 images\n",
      "Valid images: 2000\n",
      "Dataset spec before map: (TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))\n",
      "Dataset spec after map: (TensorSpec(shape=(224, 224, 3), dtype=tf.float32, name=None), {'general_anomaly': TensorSpec(shape=(1,), dtype=tf.int32, name=None), 'violence': TensorSpec(shape=(1,), dtype=tf.int32, name=None), 'property_crime': TensorSpec(shape=(1,), dtype=tf.int32, name=None), 'anomaly_type': TensorSpec(shape=(), dtype=tf.int32, name=None)})\n",
      "Images before filtering: 2000\n",
      "Dataset spec after filter: (TensorSpec(shape=(224, 224, 3), dtype=tf.float32, name=None), {'general_anomaly': TensorSpec(shape=(1,), dtype=tf.int32, name=None), 'violence': TensorSpec(shape=(1,), dtype=tf.int32, name=None), 'property_crime': TensorSpec(shape=(1,), dtype=tf.int32, name=None), 'anomaly_type': TensorSpec(shape=(), dtype=tf.int32, name=None)})\n",
      "Images after filtering: 1999\n",
      "Batch image shape: (16, 224, 224, 3)\n",
      "Batch label shapes: { general_anomaly: (16, 1), violence: (16, 1), property_crime: (16, 1), anomaly_type: (16,) }\n",
      "Sample anomaly_type values: [ 8 13 13  4 13]\n",
      "Expected steps per epoch: 125 (with batch_size=16)\n",
      "Returning tf.data.Dataset for test_annotations.txt\n",
      "Time taken: 2.16 seconds\n",
      "\n",
      "Datasets ready for training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_ucf_crime_data(data_dir, annotation_file, image_size=(224, 224), batch_size=16, max_images=None, return_numpy=False):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if not os.path.exists(annotation_file):\n",
    "        raise FileNotFoundError(f\"Annotation file {annotation_file} not found.\")\n",
    "    \n",
    "    try:\n",
    "        annotations = pd.read_csv(annotation_file, sep=' ', header=None, names=['image', 'label'], \n",
    "                                 quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "        print(f\"Loaded annotation file: {annotation_file}\")\n",
    "        print(f\"Total annotations: {len(annotations)}\")\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"Error parsing {annotation_file}: {e}\")\n",
    "        raise\n",
    "    \n",
    "    if len(annotations) == 0:\n",
    "        raise ValueError(f\"Annotation file {annotation_file} is empty.\")\n",
    "    \n",
    "    if max_images is not None and len(annotations) > max_images:\n",
    "        annotations = annotations.sample(n=max_images, random_state=42)\n",
    "        print(f\"Subsampled to {max_images} images\")\n",
    "    \n",
    "    anomaly_classes = tf.constant(['Abuse', 'Arrest', 'Arson', 'Assault', 'Burglary', 'Explosion',\n",
    "                                  'Fighting', 'Robbery', 'Shooting', 'Stealing', 'Shoplifting',\n",
    "                                  'Vandalism', 'RoadAccident'], dtype=tf.string)\n",
    "    violent_classes = tf.constant(['Assault', 'Fighting', 'Shooting'], dtype=tf.string)\n",
    "    property_classes = tf.constant(['Burglary', 'Stealing', 'Shoplifting', 'Vandalism'], dtype=tf.string)\n",
    "    \n",
    "    @tf.function\n",
    "    def process_path(image_path, label):\n",
    "        try:\n",
    "            img = tf.io.read_file(image_path)\n",
    "            img = tf.image.decode_png(img, channels=3)\n",
    "            img = tf.image.resize(img, image_size)\n",
    "            img = img / 255.0\n",
    "            img = tf.cast(img, tf.float32)\n",
    "        except tf.errors.InvalidArgumentError:\n",
    "            tf.print(\"Failed to decode image:\", image_path)\n",
    "            return tf.zeros((image_size[0], image_size[1], 3), dtype=tf.float32), {\n",
    "                'general_anomaly': tf.zeros((1,), dtype=tf.int32),\n",
    "                'violence': tf.zeros((1,), dtype=tf.int32),\n",
    "                'property_crime': tf.zeros((1,), dtype=tf.int32),\n",
    "                'anomaly_type': tf.constant(len(anomaly_classes), dtype=tf.int32)\n",
    "            }\n",
    "        \n",
    "        is_anomaly = tf.reduce_any(tf.equal(anomaly_classes, label))\n",
    "        is_anomaly = tf.cast(is_anomaly, tf.int32)\n",
    "        is_anomaly = tf.expand_dims(is_anomaly, axis=-1)\n",
    "        \n",
    "        is_violent = tf.reduce_any(tf.equal(violent_classes, label))\n",
    "        is_violent = tf.cast(is_violent, tf.int32)\n",
    "        is_violent = tf.expand_dims(is_violent, axis=-1)\n",
    "        \n",
    "        is_property = tf.reduce_any(tf.equal(property_classes, label))\n",
    "        is_property = tf.cast(is_property, tf.int32)\n",
    "        is_property = tf.expand_dims(is_property, axis=-1)\n",
    "        \n",
    "        matches = tf.equal(anomaly_classes, label)\n",
    "        indices = tf.cast(tf.where(matches), tf.int32)\n",
    "        anomaly_type = tf.cond(\n",
    "            tf.reduce_any(matches),\n",
    "            lambda: tf.reduce_max(indices),\n",
    "            lambda: tf.constant(len(anomaly_classes), dtype=tf.int32)\n",
    "        )\n",
    "        \n",
    "        return img, {\n",
    "            'general_anomaly': is_anomaly,\n",
    "            'violence': is_violent,\n",
    "            'property_crime': is_property,\n",
    "            'anomaly_type': anomaly_type\n",
    "        }\n",
    "    \n",
    "    image_paths = [os.path.normpath(os.path.join(data_dir, row['image'])) for _, row in annotations.iterrows()]\n",
    "    labels = annotations['label'].tolist()\n",
    "    \n",
    "    valid_indices = [i for i, path in enumerate(image_paths) if os.path.exists(path)]\n",
    "    if len(valid_indices) < len(image_paths):\n",
    "        print(f\"WARNING: {len(image_paths) - len(valid_indices)} invalid image paths found\")\n",
    "    \n",
    "    image_paths = [image_paths[i] for i in valid_indices]\n",
    "    labels = [labels[i] for i in valid_indices]\n",
    "    print(f\"Valid images: {len(image_paths)}\")\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    \n",
    "    print(f\"Dataset spec before map: {dataset.element_spec}\")\n",
    "    \n",
    "    dataset = dataset.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    print(f\"Dataset spec after map: {dataset.element_spec}\")\n",
    "    \n",
    "    pre_filter_count = len(image_paths)\n",
    "    print(f\"Images before filtering: {pre_filter_count}\")\n",
    "    \n",
    "    dataset = dataset.filter(lambda img, lbl: tf.reduce_any(tf.greater(img, 0)))\n",
    "    \n",
    "    print(f\"Dataset spec after filter: {dataset.element_spec}\")\n",
    "    \n",
    "    try:\n",
    "        post_filter_count = sum(1 for _ in dataset)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to count post-filter images: {e}\")\n",
    "        post_filter_count = pre_filter_count\n",
    "    print(f\"Images after filtering: {post_filter_count}\")\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    for images, labels in dataset.take(1):\n",
    "        print(f\"Batch image shape: {images.shape}\")\n",
    "        print(f\"Batch label shapes: {{ {', '.join(f'{task}: {labels[task].shape}' for task in labels)} }}\")\n",
    "        print(f\"Sample anomaly_type values: {labels['anomaly_type'][:5].numpy()}\")\n",
    "    \n",
    "    expected_steps = (post_filter_count + batch_size - 1) // batch_size\n",
    "    print(f\"Expected steps per epoch: {expected_steps} (with batch_size={batch_size})\")\n",
    "    \n",
    "    if not return_numpy:\n",
    "        print(f\"Returning tf.data.Dataset for {annotation_file}\")\n",
    "        print(f\"Time taken: {time.time() - start_time:.2f} seconds\")\n",
    "        return dataset\n",
    "    \n",
    "    images = []\n",
    "    label_dict = {\n",
    "        'general_anomaly': [],\n",
    "        'violence': [],\n",
    "        'property_crime': [],\n",
    "        'anomaly_type': []\n",
    "    }\n",
    "    \n",
    "    for img, lbl in dataset.unbatch():\n",
    "        images.append(img.numpy())\n",
    "        for task in label_dict:\n",
    "            label_dict[task].append(lbl[task].numpy())\n",
    "    \n",
    "    images = np.array(images)\n",
    "    for task in label_dict:\n",
    "        label_dict[task] = np.array(label_dict[task])\n",
    "    \n",
    "    print(f\"Returning NumPy arrays: images shape {images.shape}, label shapes {{ {', '.join(f'{task}: {label_dict[task].shape}' for task in label_dict)} }}\")\n",
    "    print(f\"Time taken: {time.time() - start_time:.2f} seconds\")\n",
    "    return images, label_dict\n",
    "\n",
    "print(\"Loading training data...\")\n",
    "try:\n",
    "    train_dataset = load_ucf_crime_data(DATA_DIR, TRAIN_ANNOTATION_FILE, IMAGE_SIZE, BATCH_SIZE, max_images=10000, return_numpy=False)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load training data: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\nLoading validation data...\")\n",
    "try:\n",
    "    val_dataset = load_ucf_crime_data(DATA_DIR, TEST_ANNOTATION_FILE, IMAGE_SIZE, BATCH_SIZE, max_images=2000, return_numpy=False)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load validation data: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\nDatasets ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3942d0ec",
   "metadata": {},
   "source": [
    "# Stage 4: Model Definition\n",
    "Define the MTL model with a ResNet50 backbone and four task-specific heads using build_mtl_model. Display the model summary for inspection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6c5d7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " resnet50 (Functional)          (None, 7, 7, 2048)   23587712    ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 2048)        0           ['resnet50[0][0]']               \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          1049088     ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " general_anomaly (Dense)        (None, 1)            513         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " violence (Dense)               (None, 1)            513         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " property_crime (Dense)         (None, 1)            513         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " anomaly_type (Dense)           (None, 14)           7182        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 24,645,521\n",
      "Trainable params: 1,057,809\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_mtl_model(input_shape=(224, 224, 3), num_anomaly_types=14):\n",
    "    \"\"\"\n",
    "    Build MTL model with shared ResNet50 backbone and task-specific heads for images.\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    x = base_model(inputs)\n",
    "    pooled = layers.GlobalAveragePooling2D()(x)\n",
    "    shared_dense = layers.Dense(512, activation='relu')(pooled)\n",
    "    \n",
    "    anomaly_output = layers.Dense(1, activation='sigmoid', name='general_anomaly')(shared_dense)\n",
    "    violence_output = layers.Dense(1, activation='sigmoid', name='violence')(shared_dense)\n",
    "    property_output = layers.Dense(1, activation='sigmoid', name='property_crime')(shared_dense)\n",
    "    type_output = layers.Dense(num_anomaly_types, activation='softmax', name='anomaly_type')(shared_dense)\n",
    "    \n",
    "    model = models.Model(inputs, [\n",
    "        anomaly_output, violence_output, property_output, type_output\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Build model and display summary\n",
    "model = build_mtl_model(input_shape=(224, 224, 3), num_anomaly_types=NUM_ANOMALY_TYPES)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4acec21",
   "metadata": {},
   "source": [
    "# Stage 5: Task Relationship Functions\n",
    "Define functions to analyze task relationships: compute_gradient_alignment (cosine similarity of gradients) and compute_loss_correlation (Pearson correlation of losses).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eda9c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 5 or Helper: Gradient Alignment\n",
    "\n",
    "def compute_gradient_alignment(model, data, labels, task_names):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between gradients of different tasks.\n",
    "    \n",
    "    Args:\n",
    "        model: Keras MTL model.\n",
    "        data: Input data batch (e.g., shape (batch_size, 224, 224, 3)).\n",
    "        labels: Dict of label tensors (e.g., {'general_anomaly': (batch_size, 1), ..., 'anomaly_type': (batch_size,)}).\n",
    "        task_names: List of task names ['general_anomaly', 'violence', 'property_crime', 'anomaly_type'].\n",
    "    \n",
    "    Returns:\n",
    "        Dict of task pair similarities (cosine similarity of gradients).\n",
    "    \"\"\"\n",
    "    gradients = {}\n",
    "    loss_functions = {\n",
    "        'general_anomaly': tf.keras.losses.BinaryCrossentropy(),\n",
    "        'violence': tf.keras.losses.BinaryCrossentropy(),\n",
    "        'property_crime': tf.keras.losses.BinaryCrossentropy(),\n",
    "        'anomaly_type': tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    }\n",
    "    \n",
    "    # Debug input shapes\n",
    "    print(f\"Input data shape: {data.shape}\")\n",
    "    print(f\"Label shapes: {{ {', '.join(f'{task}: {labels[task].shape}' for task in task_names)} }}\")\n",
    "    \n",
    "    # Get trainable variables (task-specific dense layers)\n",
    "    trainable_vars = [v for v in model.trainable_variables if 'dense' in v.name]\n",
    "    print(f\"Trainable variables: {[v.name for v in trainable_vars]}\")\n",
    "    \n",
    "    for task in task_names:\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(data)\n",
    "            task_idx = task_names.index(task)\n",
    "            loss = loss_functions[task](labels[task], predictions[task_idx])\n",
    "            grads = tape.gradient(loss, trainable_vars)\n",
    "            # Debug gradient shapes\n",
    "            grad_shapes = [g.shape.as_list() if g is not None else None for g in grads]\n",
    "            print(f\"Task {task} gradient shapes: {grad_shapes}\")\n",
    "            gradients[task] = grads\n",
    "    \n",
    "    similarities = {}\n",
    "    for i, task1 in enumerate(task_names):\n",
    "        for task2 in task_names[i+1:]:\n",
    "            grads1 = [g for g in gradients[task1] if g is not None]\n",
    "            grads2 = [g for g in gradients[task2] if g is not None]\n",
    "            if not grads1 or not grads2:\n",
    "                print(f\"Skipping {task1}_{task2}: No valid gradients\")\n",
    "                similarities[f\"{task1}_{task2}\"] = 0.0\n",
    "                continue\n",
    "            # Flatten gradients\n",
    "            flat_grads1 = tf.concat([tf.reshape(g, [-1]) for g in grads1], axis=0)\n",
    "            flat_grads2 = tf.concat([tf.reshape(g, [-1]) for g in grads2], axis=0)\n",
    "            # Verify shapes\n",
    "            if flat_grads1.shape != flat_grads2.shape:\n",
    "                print(f\"Shape mismatch for {task1}_{task2}: {flat_grads1.shape} vs {flat_grads2.shape}\")\n",
    "                similarities[f\"{task1}_{task2}\"] = 0.0\n",
    "                continue\n",
    "            cosine_sim = tf.reduce_sum(flat_grads1 * flat_grads2) / (\n",
    "                tf.sqrt(tf.reduce_sum(tf.square(flat_grads1))) * tf.sqrt(tf.reduce_sum(tf.square(flat_grads2))) + 1e-10\n",
    "            )\n",
    "            similarities[f\"{task1}_{task2}\"] = cosine_sim.numpy()\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def compute_loss_correlation(losses_dict, task_names):\n",
    "    \"\"\"\n",
    "    Compute Pearson correlation between task losses.\n",
    "    \n",
    "    Args:\n",
    "        losses_dict: Dict of loss lists per task (e.g., {'general_anomaly': [0.5, 0.4, ...], ...}).\n",
    "        task_names: List of task names.\n",
    "    \n",
    "    Returns:\n",
    "        Dict of correlations between task pairs.\n",
    "    \"\"\"\n",
    "    correlations = {}\n",
    "    for task1 in task_names:\n",
    "        for task2 in task_names:\n",
    "            if task1 >= task2:\n",
    "                continue\n",
    "            if len(losses_dict[task1]) < 2 or len(losses_dict[task2]) < 2:\n",
    "                print(f\"Skipping correlation for {task1}_vs_{task2}: insufficient loss values ({len(losses_dict[task1])}, {len(losses_dict[task2])})\")\n",
    "                correlations[f'{task1}_vs_{task2}'] = 0.0\n",
    "                continue\n",
    "            corr, _ = pearsonr(losses_dict[task1], losses_dict[task2])\n",
    "            correlations[f'{task1}_vs_{task2}'] = corr\n",
    "    print(\"Computed loss correlations:\", correlations)\n",
    "    return correlations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47639eb4",
   "metadata": {},
   "source": [
    "# Stage 6: Training Function\n",
    "Define train_mtl_model to train the MTL model, compute task relationships, and return training history. Monitor training progress via printed outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47616fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import psutil\n",
    "\n",
    "def train_mtl_model(model, train_data, train_labels, val_data, val_labels, epochs=10, batch_size=16):\n",
    "    task_names = ['general_anomaly', 'violence', 'property_crime', 'anomaly_type']\n",
    "    losses_dict = {task: [] for task in task_names}\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss={\n",
    "            'general_anomaly': tf.keras.losses.BinaryCrossentropy(),\n",
    "            'violence': tf.keras.losses.BinaryCrossentropy(),\n",
    "            'property_crime': tf.keras.losses.BinaryCrossentropy(),\n",
    "            'anomaly_type': tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        },\n",
    "        loss_weights={\n",
    "            'general_anomaly': 1.0,\n",
    "            'violence': 0.5,\n",
    "            'property_crime': 0.5,\n",
    "            'anomaly_type': 1.0\n",
    "        },\n",
    "        metrics={\n",
    "            'general_anomaly': ['accuracy', tf.keras.metrics.AUC(name='auc')],\n",
    "            'violence': ['accuracy'],\n",
    "            'property_crime': ['accuracy'],\n",
    "            'anomaly_type': ['accuracy']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"Memory used before training: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "    \n",
    "    if isinstance(train_data, tf.data.Dataset):\n",
    "        train_input = train_data\n",
    "        val_input = val_data\n",
    "        for batch_images, _ in train_data.take(1):\n",
    "            if batch_images.shape[0] != batch_size:\n",
    "                raise ValueError(f\"Train dataset batch size {batch_images.shape[0]} does not match {batch_size}\")\n",
    "        for batch_images, _ in val_data.take(1):\n",
    "            if batch_images.shape[0] != batch_size:\n",
    "                raise ValueError(f\"Validation dataset batch size {batch_images.shape[0]} does not match {batch_size}\")\n",
    "        train_cardinality = sum(1 for _ in train_data)\n",
    "        print(f\"Train dataset cardinality: {train_cardinality} batches\")\n",
    "        print(f\"Expected steps per epoch: {train_cardinality}\")\n",
    "    else:\n",
    "        train_input = train_data\n",
    "        val_input = val_data\n",
    "        train_labels = [train_labels[task] for task in task_names]\n",
    "        val_labels = [val_labels[task] for task in task_names]\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_input,\n",
    "        validation_data=val_input,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size if isinstance(train_input, np.ndarray) else None,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    for task in task_names:\n",
    "        losses_dict[task] = history.history[f'{task}_loss']\n",
    "        print(f\"Losses for {task}: {losses_dict[task]}\")\n",
    "    \n",
    "    print(f\"Memory used after training: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "    \n",
    "    if isinstance(val_data, tf.data.Dataset):\n",
    "        for batch_images, batch_labels in val_data.take(1):\n",
    "            val_batch = batch_images\n",
    "            val_labels_batch = batch_labels\n",
    "            break\n",
    "    else:\n",
    "        val_batch = val_data[:batch_size]\n",
    "        val_labels_batch = {task: val_labels[task][:batch_size] for task in task_names}\n",
    "    \n",
    "    print(f\"Memory used before gradient alignment: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "    grad_similarities = compute_gradient_alignment(model, val_batch, val_labels_batch, task_names)\n",
    "    print(f\"Memory used after gradient alignment: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "    \n",
    "    loss_correlations = compute_loss_correlation(losses_dict, task_names)\n",
    "    \n",
    "    return history, grad_similarities, loss_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2175a8b6",
   "metadata": {},
   "source": [
    "# Stage 7: Ablation Study\n",
    "Define ablation_study to evaluate the model with subsets of tasks, reporting AUC for general anomaly detection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "538f8379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablation_study(X_train, y_train, X_val, y_val, tasks_to_include, input_shape=(224, 224, 3), num_anomaly_types=14):\n",
    "    \"\"\"\n",
    "    Train model with a subset of tasks for ablation study.\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    x = base_model(inputs)\n",
    "    pooled = layers.GlobalAveragePooling2D()(x)\n",
    "    shared_dense = layers.Dense(512, activation='relu')(pooled)\n",
    "    \n",
    "    outputs = []\n",
    "    loss_dict = {}\n",
    "    metrics_dict = {}\n",
    "    \n",
    "    for task in tasks_to_include:\n",
    "        if task in ['general_anomaly', 'violence', 'property_crime']:\n",
    "            output = layers.Dense(1, activation='sigmoid', name=task)(shared_dense)\n",
    "            loss_dict[task] = 'binary_crossentropy'\n",
    "            metrics_dict[task] = ['accuracy']\n",
    "        elif task == 'anomaly_type':\n",
    "            output = layers.Dense(num_anomaly_types, activation='softmax', name=task)(shared_dense)\n",
    "            loss_dict[task] = 'sparse_categorical_crossentropy'\n",
    "            metrics_dict[task] = ['accuracy']\n",
    "        outputs.append(output)\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=loss_dict,\n",
    "        metrics=metrics_dict\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        [y_train[task] for task in tasks_to_include],\n",
    "        validation_data=(X_val, [y_val[task] for task in tasks_to_include]),\n",
    "        epochs=1,\n",
    "        batch_size=32,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    val_preds = model.predict(X_val)\n",
    "    general_idx = tasks_to_include.index('general_anomaly') if 'general_anomaly' in tasks_to_include else None\n",
    "    if general_idx is not None:\n",
    "        auc = roc_auc_score(y_val['general_anomaly'], val_preds[general_idx])\n",
    "        print(f\"AUC for tasks {tasks_to_include}: {auc:.4f}\")\n",
    "        return auc\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112054fc",
   "metadata": {},
   "source": [
    "# Stage 8: Execution and Results\n",
    "Run the pipeline, train the model, compute task relationships, and perform the ablation study. Display results for gradient alignment, loss correlation, and AUC scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7a3ecf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MTL model...\n",
      "Memory used before training: 4270.37 MB\n",
      "Train dataset cardinality: 625 batches\n",
      "Expected steps per epoch: 625\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 24s 30ms/step - loss: 1.6445 - general_anomaly_loss: 0.4713 - violence_loss: 0.1450 - property_crime_loss: 0.2894 - anomaly_type_loss: 0.9560 - general_anomaly_accuracy: 0.7756 - general_anomaly_auc: 0.7480 - violence_accuracy: 0.9640 - property_crime_accuracy: 0.9042 - anomaly_type_accuracy: 0.7663 - val_loss: 2.7541 - val_general_anomaly_loss: 0.6238 - val_violence_loss: 0.3466 - val_property_crime_loss: 0.4290 - val_anomaly_type_loss: 1.7424 - val_general_anomaly_accuracy: 0.6598 - val_general_anomaly_auc: 0.7017 - val_violence_accuracy: 0.8949 - val_property_crime_accuracy: 0.8264 - val_anomaly_type_accuracy: 0.5988\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 17s 28ms/step - loss: 1.6257 - general_anomaly_loss: 0.4662 - violence_loss: 0.1444 - property_crime_loss: 0.2859 - anomaly_type_loss: 0.9444 - general_anomaly_accuracy: 0.7774 - general_anomaly_auc: 0.7561 - violence_accuracy: 0.9640 - property_crime_accuracy: 0.9046 - anomaly_type_accuracy: 0.7674 - val_loss: 2.7657 - val_general_anomaly_loss: 0.6276 - val_violence_loss: 0.3466 - val_property_crime_loss: 0.4327 - val_anomaly_type_loss: 1.7485 - val_general_anomaly_accuracy: 0.6528 - val_general_anomaly_auc: 0.6964 - val_violence_accuracy: 0.8949 - val_property_crime_accuracy: 0.8264 - val_anomaly_type_accuracy: 0.5988\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 17s 28ms/step - loss: 1.6136 - general_anomaly_loss: 0.4634 - violence_loss: 0.1440 - property_crime_loss: 0.2846 - anomaly_type_loss: 0.9359 - general_anomaly_accuracy: 0.7779 - general_anomaly_auc: 0.7600 - violence_accuracy: 0.9640 - property_crime_accuracy: 0.9048 - anomaly_type_accuracy: 0.7682 - val_loss: 2.7685 - val_general_anomaly_loss: 0.6300 - val_violence_loss: 0.3465 - val_property_crime_loss: 0.4340 - val_anomaly_type_loss: 1.7482 - val_general_anomaly_accuracy: 0.6493 - val_general_anomaly_auc: 0.6927 - val_violence_accuracy: 0.8949 - val_property_crime_accuracy: 0.8264 - val_anomaly_type_accuracy: 0.5988\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 17s 28ms/step - loss: 1.6023 - general_anomaly_loss: 0.4609 - violence_loss: 0.1435 - property_crime_loss: 0.2827 - anomaly_type_loss: 0.9283 - general_anomaly_accuracy: 0.7782 - general_anomaly_auc: 0.7642 - violence_accuracy: 0.9640 - property_crime_accuracy: 0.9048 - anomaly_type_accuracy: 0.7695 - val_loss: 2.7886 - val_general_anomaly_loss: 0.6314 - val_violence_loss: 0.3455 - val_property_crime_loss: 0.4364 - val_anomaly_type_loss: 1.7662 - val_general_anomaly_accuracy: 0.6653 - val_general_anomaly_auc: 0.6900 - val_violence_accuracy: 0.8949 - val_property_crime_accuracy: 0.8264 - val_anomaly_type_accuracy: 0.5988\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 17s 27ms/step - loss: 1.5878 - general_anomaly_loss: 0.4575 - violence_loss: 0.1426 - property_crime_loss: 0.2810 - anomaly_type_loss: 0.9185 - general_anomaly_accuracy: 0.7799 - general_anomaly_auc: 0.7686 - violence_accuracy: 0.9640 - property_crime_accuracy: 0.9051 - anomaly_type_accuracy: 0.7710 - val_loss: 2.7940 - val_general_anomaly_loss: 0.6304 - val_violence_loss: 0.3478 - val_property_crime_loss: 0.4377 - val_anomaly_type_loss: 1.7708 - val_general_anomaly_accuracy: 0.6658 - val_general_anomaly_auc: 0.6934 - val_violence_accuracy: 0.8949 - val_property_crime_accuracy: 0.8264 - val_anomaly_type_accuracy: 0.5988\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 17s 27ms/step - loss: 1.5787 - general_anomaly_loss: 0.4558 - violence_loss: 0.1425 - property_crime_loss: 0.2801 - anomaly_type_loss: 0.9116 - general_anomaly_accuracy: 0.7789 - general_anomaly_auc: 0.7712 - violence_accuracy: 0.9640 - property_crime_accuracy: 0.9053 - anomaly_type_accuracy: 0.7715 - val_loss: 2.8056 - val_general_anomaly_loss: 0.6333 - val_violence_loss: 0.3475 - val_property_crime_loss: 0.4383 - val_anomaly_type_loss: 1.7794 - val_general_anomaly_accuracy: 0.6878 - val_general_anomaly_auc: 0.6903 - val_violence_accuracy: 0.8949 - val_property_crime_accuracy: 0.8264 - val_anomaly_type_accuracy: 0.5973\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 17s 28ms/step - loss: 1.5653 - general_anomaly_loss: 0.4526 - violence_loss: 0.1419 - property_crime_loss: 0.2785 - anomaly_type_loss: 0.9025 - general_anomaly_accuracy: 0.7812 - general_anomaly_auc: 0.7753 - violence_accuracy: 0.9640 - property_crime_accuracy: 0.9054 - anomaly_type_accuracy: 0.7728 - val_loss: 2.8528 - val_general_anomaly_loss: 0.6381 - val_violence_loss: 0.3491 - val_property_crime_loss: 0.4440 - val_anomaly_type_loss: 1.8182 - val_general_anomaly_accuracy: 0.6778 - val_general_anomaly_auc: 0.6784 - val_violence_accuracy: 0.8949 - val_property_crime_accuracy: 0.8264 - val_anomaly_type_accuracy: 0.5963\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 17s 27ms/step - loss: 1.5507 - general_anomaly_loss: 0.4489 - violence_loss: 0.1413 - property_crime_loss: 0.2766 - anomaly_type_loss: 0.8928 - general_anomaly_accuracy: 0.7829 - general_anomaly_auc: 0.7802 - violence_accuracy: 0.9640 - property_crime_accuracy: 0.9054 - anomaly_type_accuracy: 0.7745 - val_loss: 2.8479 - val_general_anomaly_loss: 0.6391 - val_violence_loss: 0.3487 - val_property_crime_loss: 0.4486 - val_anomaly_type_loss: 1.8102 - val_general_anomaly_accuracy: 0.6868 - val_general_anomaly_auc: 0.6835 - val_violence_accuracy: 0.8949 - val_property_crime_accuracy: 0.8264 - val_anomaly_type_accuracy: 0.5978\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 17s 28ms/step - loss: 1.5406 - general_anomaly_loss: 0.4473 - violence_loss: 0.1411 - property_crime_loss: 0.2747 - anomaly_type_loss: 0.8854 - general_anomaly_accuracy: 0.7836 - general_anomaly_auc: 0.7826 - violence_accuracy: 0.9640 - property_crime_accuracy: 0.9055 - anomaly_type_accuracy: 0.7758 - val_loss: 2.8681 - val_general_anomaly_loss: 0.6390 - val_violence_loss: 0.3490 - val_property_crime_loss: 0.4485 - val_anomaly_type_loss: 1.8303 - val_general_anomaly_accuracy: 0.6903 - val_general_anomaly_auc: 0.6825 - val_violence_accuracy: 0.8949 - val_property_crime_accuracy: 0.8264 - val_anomaly_type_accuracy: 0.5973\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 17s 28ms/step - loss: 1.5257 - general_anomaly_loss: 0.4433 - violence_loss: 0.1403 - property_crime_loss: 0.2733 - anomaly_type_loss: 0.8757 - general_anomaly_accuracy: 0.7845 - general_anomaly_auc: 0.7879 - violence_accuracy: 0.9640 - property_crime_accuracy: 0.9054 - anomaly_type_accuracy: 0.7765 - val_loss: 2.9142 - val_general_anomaly_loss: 0.6437 - val_violence_loss: 0.3500 - val_property_crime_loss: 0.4501 - val_anomaly_type_loss: 1.8704 - val_general_anomaly_accuracy: 0.6348 - val_general_anomaly_auc: 0.6750 - val_violence_accuracy: 0.8949 - val_property_crime_accuracy: 0.8264 - val_anomaly_type_accuracy: 0.5918\n",
      "Losses for general_anomaly: [0.4712885022163391, 0.46616771817207336, 0.4634087383747101, 0.46085324883461, 0.45751523971557617, 0.455779492855072, 0.4526229798793793, 0.4489486515522003, 0.4472518563270569, 0.4432506561279297]\n",
      "Losses for violence: [0.1450326144695282, 0.1443975567817688, 0.1440262347459793, 0.14347083866596222, 0.1426093429327011, 0.1425132304430008, 0.14186587929725647, 0.1413002610206604, 0.14110170304775238, 0.14025355875492096]\n",
      "Losses for property_crime: [0.28936004638671875, 0.2858944535255432, 0.28459295630455017, 0.2827182114124298, 0.28102394938468933, 0.28008875250816345, 0.278525173664093, 0.276583194732666, 0.27473190426826477, 0.2732824683189392]\n",
      "Losses for anomaly_type: [0.9559810161590576, 0.9443550705909729, 0.9358764886856079, 0.9283251762390137, 0.9184910655021667, 0.9115698337554932, 0.902500331401825, 0.8927909135818481, 0.8854461908340454, 0.8756729364395142]\n",
      "Memory used after training: 4387.23 MB\n",
      "Memory used before gradient alignment: 4387.23 MB\n",
      "Input data shape: (16, 224, 224, 3)\n",
      "Label shapes: { general_anomaly: (16, 1), violence: (16, 1), property_crime: (16, 1), anomaly_type: (16,) }\n",
      "Trainable variables: ['dense/kernel:0', 'dense/bias:0']\n",
      "Task general_anomaly gradient shapes: [[2048, 512], [512]]\n",
      "Task violence gradient shapes: [[2048, 512], [512]]\n",
      "Task property_crime gradient shapes: [[2048, 512], [512]]\n",
      "Task anomaly_type gradient shapes: [[2048, 512], [512]]\n",
      "Memory used after gradient alignment: 4387.36 MB\n",
      "Computed loss correlations: {'general_anomaly_vs_violence': 0.9968273683538582, 'general_anomaly_vs_property_crime': 0.9982476501026294, 'property_crime_vs_violence': 0.9929737782997057, 'anomaly_type_vs_general_anomaly': 0.9988957068313015, 'anomaly_type_vs_violence': 0.996880547847472, 'anomaly_type_vs_property_crime': 0.996993983103357}\n",
      "\n",
      "Gradient Alignment (Cosine Similarity):\n",
      "general_anomaly_violence: -0.1796\n",
      "general_anomaly_property_crime: 0.4625\n",
      "general_anomaly_anomaly_type: 0.1894\n",
      "violence_property_crime: -0.1028\n",
      "violence_anomaly_type: 0.2848\n",
      "property_crime_anomaly_type: -0.0892\n",
      "\n",
      "Loss Correlation (Pearson):\n",
      "general_anomaly_vs_violence: 0.9968\n",
      "general_anomaly_vs_property_crime: 0.9982\n",
      "property_crime_vs_violence: 0.9930\n",
      "anomaly_type_vs_general_anomaly: 0.9989\n",
      "anomaly_type_vs_violence: 0.9969\n",
      "anomaly_type_vs_property_crime: 0.9970\n",
      "\n",
      "Ablation Study Results (AUC for General Anomaly Detection):\n",
      "Ablation train batch: images shape (16, 224, 224, 3), labels { general_anomaly: (16, 1) }\n",
      "Ablation val batch: images shape (16, 224, 224, 3), labels { general_anomaly: (16, 1) }\n",
      "Model output shapes:\n",
      "general_anomaly: (None, 1)\n",
      "Epoch 1/5\n",
      "625/625 [==============================] - 20s 25ms/step - loss: 0.5596 - accuracy: 0.7596 - val_loss: 0.6417 - val_accuracy: 0.6003\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 14s 23ms/step - loss: 0.5260 - accuracy: 0.7648 - val_loss: 0.6401 - val_accuracy: 0.6043\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 14s 23ms/step - loss: 0.5164 - accuracy: 0.7645 - val_loss: 0.6316 - val_accuracy: 0.6093\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 14s 23ms/step - loss: 0.5094 - accuracy: 0.7651 - val_loss: 0.6259 - val_accuracy: 0.6048\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 14s 23ms/step - loss: 0.5034 - accuracy: 0.7657 - val_loss: 0.6235 - val_accuracy: 0.6133\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "val_true shape: (16, 1)\n",
      "val_preds shapes: [(16, 1)]\n",
      "y_score shape: (16,)\n",
      "y_true shape: (16,)\n",
      "AUC for tasks ['general_anomaly']: 0.8889\n",
      "Tasks: ['general_anomaly'], AUC: 0.8889\n",
      "Ablation train batch: images shape (16, 224, 224, 3), labels { general_anomaly: (16, 1), violence: (16, 1), property_crime: (16, 1) }\n",
      "Ablation val batch: images shape (16, 224, 224, 3), labels { general_anomaly: (16, 1), violence: (16, 1), property_crime: (16, 1) }\n",
      "Model output shapes:\n",
      "general_anomaly: (None, 1)\n",
      "violence: (None, 1)\n",
      "property_crime: (None, 1)\n",
      "Epoch 1/5\n",
      "625/625 [==============================] - 23s 29ms/step - loss: 1.0464 - general_anomaly_loss: 0.5505 - property_crime_loss: 0.3315 - violence_loss: 0.1644 - general_anomaly_accuracy: 0.7636 - property_crime_accuracy: 0.9041 - violence_accuracy: 0.9628 - val_loss: 1.4603 - val_general_anomaly_loss: 0.6496 - val_property_crime_loss: 0.4750 - val_violence_loss: 0.3357 - val_general_anomaly_accuracy: 0.5988 - val_property_crime_accuracy: 0.8264 - val_violence_accuracy: 0.8949\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 17s 26ms/step - loss: 0.9977 - general_anomaly_loss: 0.5270 - property_crime_loss: 0.3151 - violence_loss: 0.1556 - general_anomaly_accuracy: 0.7652 - property_crime_accuracy: 0.9041 - violence_accuracy: 0.9640 - val_loss: 1.4407 - val_general_anomaly_loss: 0.6457 - val_property_crime_loss: 0.4621 - val_violence_loss: 0.3329 - val_general_anomaly_accuracy: 0.5993 - val_property_crime_accuracy: 0.8264 - val_violence_accuracy: 0.8949\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 17s 26ms/step - loss: 0.9828 - general_anomaly_loss: 0.5187 - property_crime_loss: 0.3106 - violence_loss: 0.1536 - general_anomaly_accuracy: 0.7643 - property_crime_accuracy: 0.9041 - violence_accuracy: 0.9640 - val_loss: 1.4209 - val_general_anomaly_loss: 0.6383 - val_property_crime_loss: 0.4486 - val_violence_loss: 0.3340 - val_general_anomaly_accuracy: 0.6078 - val_property_crime_accuracy: 0.8264 - val_violence_accuracy: 0.8949\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 17s 26ms/step - loss: 0.9705 - general_anomaly_loss: 0.5113 - property_crime_loss: 0.3069 - violence_loss: 0.1523 - general_anomaly_accuracy: 0.7639 - property_crime_accuracy: 0.9041 - violence_accuracy: 0.9640 - val_loss: 1.4064 - val_general_anomaly_loss: 0.6326 - val_property_crime_loss: 0.4395 - val_violence_loss: 0.3344 - val_general_anomaly_accuracy: 0.6063 - val_property_crime_accuracy: 0.8264 - val_violence_accuracy: 0.8949\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 17s 26ms/step - loss: 0.9588 - general_anomaly_loss: 0.5041 - property_crime_loss: 0.3036 - violence_loss: 0.1511 - general_anomaly_accuracy: 0.7651 - property_crime_accuracy: 0.9041 - violence_accuracy: 0.9640 - val_loss: 1.3974 - val_general_anomaly_loss: 0.6282 - val_property_crime_loss: 0.4356 - val_violence_loss: 0.3337 - val_general_anomaly_accuracy: 0.6023 - val_property_crime_accuracy: 0.8264 - val_violence_accuracy: 0.8949\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "val_true shape: (16, 1)\n",
      "val_preds shapes: [(16, 1), (16, 1), (16, 1)]\n",
      "y_score shape: (16,)\n",
      "y_true shape: (16,)\n",
      "AUC for tasks ['general_anomaly', 'violence', 'property_crime']: 0.8413\n",
      "Tasks: ['general_anomaly', 'violence', 'property_crime'], AUC: 0.8413\n",
      "Ablation train batch: images shape (16, 224, 224, 3), labels { general_anomaly: (16, 1), anomaly_type: (16,) }\n",
      "Ablation val batch: images shape (16, 224, 224, 3), labels { general_anomaly: (16, 1), anomaly_type: (16,) }\n",
      "Model output shapes:\n",
      "general_anomaly: (None, 1)\n",
      "anomaly_type: (None, 14)\n",
      "Epoch 1/5\n",
      "625/625 [==============================] - 21s 27ms/step - loss: 1.6786 - anomaly_type_loss: 1.1328 - general_anomaly_loss: 0.5458 - anomaly_type_accuracy: 0.7640 - general_anomaly_accuracy: 0.7654 - val_loss: 2.3857 - val_anomaly_type_loss: 1.7003 - val_general_anomaly_loss: 0.6854 - val_anomaly_type_accuracy: 0.5988 - val_general_anomaly_accuracy: 0.5988\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 15s 24ms/step - loss: 1.6069 - anomaly_type_loss: 1.0795 - general_anomaly_loss: 0.5274 - anomaly_type_accuracy: 0.7654 - general_anomaly_accuracy: 0.7655 - val_loss: 2.3361 - val_anomaly_type_loss: 1.6831 - val_general_anomaly_loss: 0.6530 - val_anomaly_type_accuracy: 0.5988 - val_general_anomaly_accuracy: 0.5988\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 15s 24ms/step - loss: 1.5767 - anomaly_type_loss: 1.0600 - general_anomaly_loss: 0.5166 - anomaly_type_accuracy: 0.7654 - general_anomaly_accuracy: 0.7648 - val_loss: 2.3255 - val_anomaly_type_loss: 1.6850 - val_general_anomaly_loss: 0.6405 - val_anomaly_type_accuracy: 0.5988 - val_general_anomaly_accuracy: 0.6048\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 15s 24ms/step - loss: 1.5529 - anomaly_type_loss: 1.0444 - general_anomaly_loss: 0.5085 - anomaly_type_accuracy: 0.7654 - general_anomaly_accuracy: 0.7651 - val_loss: 2.3214 - val_anomaly_type_loss: 1.6881 - val_general_anomaly_loss: 0.6333 - val_anomaly_type_accuracy: 0.5988 - val_general_anomaly_accuracy: 0.6058\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 16s 25ms/step - loss: 1.5335 - anomaly_type_loss: 1.0316 - general_anomaly_loss: 0.5019 - anomaly_type_accuracy: 0.7655 - general_anomaly_accuracy: 0.7656 - val_loss: 2.3080 - val_anomaly_type_loss: 1.6815 - val_general_anomaly_loss: 0.6266 - val_anomaly_type_accuracy: 0.5988 - val_general_anomaly_accuracy: 0.6078\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "val_true shape: (16, 1)\n",
      "val_preds shapes: [(16, 1), (16, 14)]\n",
      "y_score shape: (16,)\n",
      "y_true shape: (16,)\n",
      "AUC for tasks ['general_anomaly', 'anomaly_type']: 0.8413\n",
      "Tasks: ['general_anomaly', 'anomaly_type'], AUC: 0.8413\n",
      "Ablation train batch: images shape (16, 224, 224, 3), labels { general_anomaly: (16, 1), violence: (16, 1), property_crime: (16, 1), anomaly_type: (16,) }\n",
      "Ablation val batch: images shape (16, 224, 224, 3), labels { general_anomaly: (16, 1), violence: (16, 1), property_crime: (16, 1), anomaly_type: (16,) }\n",
      "Model output shapes:\n",
      "general_anomaly: (None, 1)\n",
      "violence: (None, 1)\n",
      "property_crime: (None, 1)\n",
      "anomaly_type: (None, 14)\n",
      "Epoch 1/5\n",
      "625/625 [==============================] - 24s 31ms/step - loss: 2.1505 - anomaly_type_loss: 1.1241 - general_anomaly_loss: 0.5444 - property_crime_loss: 0.3232 - violence_loss: 0.1587 - anomaly_type_accuracy: 0.7640 - general_anomaly_accuracy: 0.7642 - property_crime_accuracy: 0.9025 - violence_accuracy: 0.9640 - val_loss: 3.1585 - val_anomaly_type_loss: 1.6785 - val_general_anomaly_loss: 0.6782 - val_property_crime_loss: 0.4657 - val_violence_loss: 0.3361 - val_anomaly_type_accuracy: 0.5988 - val_general_anomaly_accuracy: 0.5988 - val_property_crime_accuracy: 0.8264 - val_violence_accuracy: 0.8949\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 17s 28ms/step - loss: 2.0777 - anomaly_type_loss: 1.0811 - general_anomaly_loss: 0.5289 - property_crime_loss: 0.3136 - violence_loss: 0.1540 - anomaly_type_accuracy: 0.7654 - general_anomaly_accuracy: 0.7654 - property_crime_accuracy: 0.9041 - violence_accuracy: 0.9640 - val_loss: 3.1139 - val_anomaly_type_loss: 1.6653 - val_general_anomaly_loss: 0.6587 - val_property_crime_loss: 0.4538 - val_violence_loss: 0.3361 - val_anomaly_type_accuracy: 0.5988 - val_general_anomaly_accuracy: 0.5988 - val_property_crime_accuracy: 0.8264 - val_violence_accuracy: 0.8949\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 17s 28ms/step - loss: 2.0422 - anomaly_type_loss: 1.0617 - general_anomaly_loss: 0.5178 - property_crime_loss: 0.3100 - violence_loss: 0.1527 - anomaly_type_accuracy: 0.7654 - general_anomaly_accuracy: 0.7648 - property_crime_accuracy: 0.9041 - violence_accuracy: 0.9640 - val_loss: 3.0734 - val_anomaly_type_loss: 1.6532 - val_general_anomaly_loss: 0.6416 - val_property_crime_loss: 0.4444 - val_violence_loss: 0.3342 - val_anomaly_type_accuracy: 0.5988 - val_general_anomaly_accuracy: 0.6033 - val_property_crime_accuracy: 0.8264 - val_violence_accuracy: 0.8949\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 17s 28ms/step - loss: 2.0102 - anomaly_type_loss: 1.0440 - general_anomaly_loss: 0.5087 - property_crime_loss: 0.3064 - violence_loss: 0.1511 - anomaly_type_accuracy: 0.7654 - general_anomaly_accuracy: 0.7641 - property_crime_accuracy: 0.9041 - violence_accuracy: 0.9640 - val_loss: 3.0728 - val_anomaly_type_loss: 1.6605 - val_general_anomaly_loss: 0.6370 - val_property_crime_loss: 0.4384 - val_violence_loss: 0.3370 - val_anomaly_type_accuracy: 0.5988 - val_general_anomaly_accuracy: 0.6158 - val_property_crime_accuracy: 0.8264 - val_violence_accuracy: 0.8949\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 17s 28ms/step - loss: 1.9857 - anomaly_type_loss: 1.0302 - general_anomaly_loss: 0.5021 - property_crime_loss: 0.3034 - violence_loss: 0.1501 - anomaly_type_accuracy: 0.7654 - general_anomaly_accuracy: 0.7659 - property_crime_accuracy: 0.9041 - violence_accuracy: 0.9640 - val_loss: 3.0708 - val_anomaly_type_loss: 1.6654 - val_general_anomaly_loss: 0.6336 - val_property_crime_loss: 0.4346 - val_violence_loss: 0.3372 - val_anomaly_type_accuracy: 0.5988 - val_general_anomaly_accuracy: 0.5993 - val_property_crime_accuracy: 0.8264 - val_violence_accuracy: 0.8949\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001AB177E70A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "val_true shape: (16, 1)\n",
      "val_preds shapes: [(16, 1), (16, 1), (16, 1), (16, 14)]\n",
      "y_score shape: (16,)\n",
      "y_true shape: (16,)\n",
      "AUC for tasks ['general_anomaly', 'violence', 'property_crime', 'anomaly_type']: 0.8254\n",
      "Tasks: ['general_anomaly', 'violence', 'property_crime', 'anomaly_type'], AUC: 0.8254\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 16  # Ensure 625 steps for 10000 images\n",
    "EPOCHS = 10\n",
    "\n",
    "print(\"Training MTL model...\")\n",
    "history, grad_similarities, loss_correlations = train_mtl_model(\n",
    "    model, train_dataset, None, val_dataset, None, epochs=EPOCHS, batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(\"\\nGradient Alignment (Cosine Similarity):\")\n",
    "for pair, sim in grad_similarities.items():\n",
    "    print(f\"{pair}: {sim:.4f}\")\n",
    "\n",
    "print(\"\\nLoss Correlation (Pearson):\")\n",
    "for pair, corr in loss_correlations.items():\n",
    "    print(f\"{pair}: {corr:.4f}\")\n",
    "\n",
    "def ablation_study(train_dataset, val_dataset, tasks_to_include, input_shape=(224, 224, 3), num_anomaly_types=14):\n",
    "    # Filter dataset labels to match tasks_to_include\n",
    "    def filter_labels(image, labels):\n",
    "        filtered_labels = {task: labels[task] for task in tasks_to_include}\n",
    "        return image, filtered_labels\n",
    "    \n",
    "    # Re-batch to ensure correct batch size\n",
    "    filtered_train_dataset = train_dataset.map(filter_labels, num_parallel_calls=tf.data.AUTOTUNE).unbatch().batch(BATCH_SIZE)\n",
    "    filtered_val_dataset = val_dataset.map(filter_labels, num_parallel_calls=tf.data.AUTOTUNE).unbatch().batch(BATCH_SIZE)\n",
    "    \n",
    "    # Verify filtered dataset\n",
    "    for images, labels in filtered_train_dataset.take(1):\n",
    "        print(f\"Ablation train batch: images shape {images.shape}, labels {{ {', '.join(f'{task}: {labels[task].shape}' for task in labels)} }}\")\n",
    "    for images, labels in filtered_val_dataset.take(1):\n",
    "        print(f\"Ablation val batch: images shape {images.shape}, labels {{ {', '.join(f'{task}: {labels[task].shape}' for task in labels)} }}\")\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    x = base_model(inputs)\n",
    "    pooled = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    shared_dense = tf.keras.layers.Dense(512, activation='relu')(pooled)\n",
    "    \n",
    "    outputs = {}\n",
    "    loss_dict = {}\n",
    "    metrics_dict = {}\n",
    "    \n",
    "    for task in tasks_to_include:\n",
    "        if task in ['general_anomaly', 'violence', 'property_crime']:\n",
    "            output = tf.keras.layers.Dense(1, activation='sigmoid', name=task)(shared_dense)\n",
    "            loss_dict[task] = tf.keras.losses.BinaryCrossentropy()\n",
    "            metrics_dict[task] = ['accuracy']\n",
    "        elif task == 'anomaly_type':\n",
    "            output = tf.keras.layers.Dense(num_anomaly_types, activation='softmax', name=task)(shared_dense)\n",
    "            loss_dict[task] = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "            metrics_dict[task] = ['accuracy']\n",
    "        outputs[task] = output\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Debug model output shapes\n",
    "    print(\"Model output shapes:\")\n",
    "    for output_name, output_tensor in model.output.items():\n",
    "        print(f\"{output_name}: {output_tensor.shape}\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=loss_dict,\n",
    "        metrics=metrics_dict\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        filtered_train_dataset,\n",
    "        validation_data=filtered_val_dataset,\n",
    "        epochs=5,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    for batch_images, batch_labels in filtered_val_dataset.take(1):\n",
    "        print(f\"Prediction batch_images shape: {batch_images.shape}\")\n",
    "        val_preds = model.predict(batch_images)\n",
    "        val_true = batch_labels['general_anomaly'].numpy() if 'general_anomaly' in batch_labels else None\n",
    "        # Handle dict output from model.predict\n",
    "        if isinstance(val_preds, dict):\n",
    "            val_preds = [val_preds[task] for task in tasks_to_include]\n",
    "        # Debug prediction shapes\n",
    "        print(f\"val_true shape: {val_true.shape if val_true is not None else None}\")\n",
    "        print(f\"val_preds shapes: {[pred.shape for pred in val_preds]}\")\n",
    "        break\n",
    "    \n",
    "    general_idx = tasks_to_include.index('general_anomaly') if 'general_anomaly' in tasks_to_include else None\n",
    "    if general_idx is not None and val_true is not None:\n",
    "        # Ensure val_preds[general_idx] is flattened to match val_true\n",
    "        y_score = val_preds[general_idx].flatten()\n",
    "        print(f\"y_score shape: {y_score.shape}\")\n",
    "        # Flatten val_true to 1D\n",
    "        y_true = val_true.flatten()\n",
    "        print(f\"y_true shape: {y_true.shape}\")\n",
    "        auc = roc_auc_score(y_true, y_score)\n",
    "        print(f\"AUC for tasks {tasks_to_include}: {auc:.4f}\")\n",
    "        return auc\n",
    "    return None\n",
    "\n",
    "print(\"\\nAblation Study Results (AUC for General Anomaly Detection):\")\n",
    "task_combinations = [\n",
    "    ['general_anomaly'],\n",
    "    ['general_anomaly', 'violence', 'property_crime'],\n",
    "    ['general_anomaly', 'anomaly_type'],\n",
    "    ['general_anomaly', 'violence', 'property_crime', 'anomaly_type']\n",
    "]\n",
    "\n",
    "for tasks in task_combinations:\n",
    "    auc = ablation_study(train_dataset, val_dataset, tasks)\n",
    "    if auc is not None:\n",
    "        print(f\"Tasks: {tasks}, AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b8ab55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extended Ablation Study Results (Individual AUC Scores):\n",
      "Extended ablation train batch: images shape (16, 224, 224, 3), labels { general_anomaly: (16, 1) }\n",
      "Extended ablation val batch: images shape (16, 224, 224, 3), labels { general_anomaly: (16, 1) }\n",
      "Model output shapes:\n",
      "general_anomaly: (None, 1)\n",
      "Epoch 1/5\n",
      "625/625 [==============================] - 19s 25ms/step - loss: 0.5574 - accuracy: 0.7605 - val_loss: 0.6517 - val_accuracy: 0.5988\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 14s 23ms/step - loss: 0.5262 - accuracy: 0.7647 - val_loss: 0.6403 - val_accuracy: 0.6038\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 14s 23ms/step - loss: 0.5168 - accuracy: 0.7644 - val_loss: 0.6347 - val_accuracy: 0.6123\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 14s 23ms/step - loss: 0.5099 - accuracy: 0.7653 - val_loss: 0.6255 - val_accuracy: 0.6068\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 14s 23ms/step - loss: 0.5035 - accuracy: 0.7655 - val_loss: 0.6228 - val_accuracy: 0.6133\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001AB280D9990> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (15, 224, 224, 3)\n",
      "general_anomaly - y_true shape: (1999, 1), y_score shape: (1999, 1)\n",
      "\n",
      "AUC Scores for tasks ['general_anomaly']:\n",
      "- general_anomaly: 0.7108\n",
      "Extended ablation train batch: images shape (16, 224, 224, 3), labels { general_anomaly: (16, 1), violence: (16, 1), property_crime: (16, 1) }\n",
      "Extended ablation val batch: images shape (16, 224, 224, 3), labels { general_anomaly: (16, 1), violence: (16, 1), property_crime: (16, 1) }\n",
      "Model output shapes:\n",
      "general_anomaly: (None, 1)\n",
      "violence: (None, 1)\n",
      "property_crime: (None, 1)\n",
      "Epoch 1/5\n",
      "625/625 [==============================] - 24s 30ms/step - loss: 1.0416 - general_anomaly_loss: 0.5531 - property_crime_loss: 0.3277 - violence_loss: 0.1608 - general_anomaly_accuracy: 0.7618 - property_crime_accuracy: 0.9041 - violence_accuracy: 0.9628 - val_loss: 1.4542 - val_general_anomaly_loss: 0.6474 - val_property_crime_loss: 0.4713 - val_violence_loss: 0.3355 - val_general_anomaly_accuracy: 0.5988 - val_property_crime_accuracy: 0.8264 - val_violence_accuracy: 0.8949\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 17s 28ms/step - loss: 0.9968 - general_anomaly_loss: 0.5273 - property_crime_loss: 0.3141 - violence_loss: 0.1555 - general_anomaly_accuracy: 0.7645 - property_crime_accuracy: 0.9041 - violence_accuracy: 0.9640 - val_loss: 1.4349 - val_general_anomaly_loss: 0.6485 - val_property_crime_loss: 0.4526 - val_violence_loss: 0.3337 - val_general_anomaly_accuracy: 0.6013 - val_property_crime_accuracy: 0.8264 - val_violence_accuracy: 0.8949\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 17s 28ms/step - loss: 0.9813 - general_anomaly_loss: 0.5180 - property_crime_loss: 0.3098 - violence_loss: 0.1534 - general_anomaly_accuracy: 0.7646 - property_crime_accuracy: 0.9041 - violence_accuracy: 0.9640 - val_loss: 1.4078 - val_general_anomaly_loss: 0.6349 - val_property_crime_loss: 0.4399 - val_violence_loss: 0.3330 - val_general_anomaly_accuracy: 0.6128 - val_property_crime_accuracy: 0.8264 - val_violence_accuracy: 0.8949\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 17s 28ms/step - loss: 0.9687 - general_anomaly_loss: 0.5101 - property_crime_loss: 0.3064 - violence_loss: 0.1521 - general_anomaly_accuracy: 0.7647 - property_crime_accuracy: 0.9041 - violence_accuracy: 0.9640 - val_loss: 1.4001 - val_general_anomaly_loss: 0.6323 - val_property_crime_loss: 0.4347 - val_violence_loss: 0.3331 - val_general_anomaly_accuracy: 0.6053 - val_property_crime_accuracy: 0.8264 - val_violence_accuracy: 0.8949\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 17s 28ms/step - loss: 0.9583 - general_anomaly_loss: 0.5037 - property_crime_loss: 0.3034 - violence_loss: 0.1512 - general_anomaly_accuracy: 0.7660 - property_crime_accuracy: 0.9041 - violence_accuracy: 0.9640 - val_loss: 1.3917 - val_general_anomaly_loss: 0.6277 - val_property_crime_loss: 0.4315 - val_violence_loss: 0.3325 - val_general_anomaly_accuracy: 0.6053 - val_property_crime_accuracy: 0.8264 - val_violence_accuracy: 0.8949\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (15, 224, 224, 3)\n",
      "general_anomaly - y_true shape: (1999, 1), y_score shape: (1999, 1)\n",
      "violence - y_true shape: (1999, 1), y_score shape: (1999, 1)\n",
      "property_crime - y_true shape: (1999, 1), y_score shape: (1999, 1)\n",
      "\n",
      "AUC Scores for tasks ['general_anomaly', 'violence', 'property_crime']:\n",
      "- general_anomaly: 0.7062\n",
      "- violence: 0.5666\n",
      "- property_crime: 0.7386\n",
      "Extended ablation train batch: images shape (16, 224, 224, 3), labels { general_anomaly: (16, 1), anomaly_type: (16,) }\n",
      "Extended ablation val batch: images shape (16, 224, 224, 3), labels { general_anomaly: (16, 1), anomaly_type: (16,) }\n",
      "Model output shapes:\n",
      "general_anomaly: (None, 1)\n",
      "anomaly_type: (None, 14)\n",
      "Epoch 1/5\n",
      "625/625 [==============================] - 21s 27ms/step - loss: 1.6751 - anomaly_type_loss: 1.1284 - general_anomaly_loss: 0.5467 - anomaly_type_accuracy: 0.7640 - general_anomaly_accuracy: 0.7642 - val_loss: 2.3780 - val_anomaly_type_loss: 1.7032 - val_general_anomaly_loss: 0.6747 - val_anomaly_type_accuracy: 0.5988 - val_general_anomaly_accuracy: 0.5988\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 15s 24ms/step - loss: 1.6057 - anomaly_type_loss: 1.0786 - general_anomaly_loss: 0.5271 - anomaly_type_accuracy: 0.7654 - general_anomaly_accuracy: 0.7655 - val_loss: 2.3251 - val_anomaly_type_loss: 1.6753 - val_general_anomaly_loss: 0.6498 - val_anomaly_type_accuracy: 0.5988 - val_general_anomaly_accuracy: 0.5988\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 15s 24ms/step - loss: 1.5741 - anomaly_type_loss: 1.0574 - general_anomaly_loss: 0.5168 - anomaly_type_accuracy: 0.7654 - general_anomaly_accuracy: 0.7652 - val_loss: 2.3027 - val_anomaly_type_loss: 1.6649 - val_general_anomaly_loss: 0.6377 - val_anomaly_type_accuracy: 0.5988 - val_general_anomaly_accuracy: 0.6003\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 15s 24ms/step - loss: 1.5498 - anomaly_type_loss: 1.0411 - general_anomaly_loss: 0.5088 - anomaly_type_accuracy: 0.7654 - general_anomaly_accuracy: 0.7649 - val_loss: 2.3032 - val_anomaly_type_loss: 1.6689 - val_general_anomaly_loss: 0.6343 - val_anomaly_type_accuracy: 0.5988 - val_general_anomaly_accuracy: 0.6098\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 15s 24ms/step - loss: 1.5273 - anomaly_type_loss: 1.0258 - general_anomaly_loss: 0.5016 - anomaly_type_accuracy: 0.7655 - general_anomaly_accuracy: 0.7663 - val_loss: 2.3022 - val_anomaly_type_loss: 1.6788 - val_general_anomaly_loss: 0.6234 - val_anomaly_type_accuracy: 0.5988 - val_general_anomaly_accuracy: 0.6163\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (16, 224, 224, 3)\n",
      "Prediction batch_images shape: (15, 224, 224, 3)\n",
      "general_anomaly - y_true shape: (1999, 1), y_score shape: (1999, 1)\n",
      "anomaly_type - y_true shape: (1999,), y_score shape: (1999, 14)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes in y_true not equal to the number of columns in 'y_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 116\u001b[0m\n\u001b[0;32m    113\u001b[0m all_auc_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tasks \u001b[38;5;129;01min\u001b[39;00m task_combinations:\n\u001b[1;32m--> 116\u001b[0m     auc_scores \u001b[38;5;241m=\u001b[39m \u001b[43mextended_ablation_study\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m     all_auc_scores\u001b[38;5;241m.\u001b[39mappend((tasks, auc_scores))\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Print summary table\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 94\u001b[0m, in \u001b[0;36mextended_ablation_study\u001b[1;34m(train_dataset, val_dataset, tasks_to_include, input_shape, num_anomaly_types)\u001b[0m\n\u001b[0;32m     91\u001b[0m         auc_scores[task] \u001b[38;5;241m=\u001b[39m auc\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manomaly_type\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;66;03m# Multi-class AUC (one-vs-rest, macro-averaged)\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m         auc \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43movr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m         auc_scores[task] \u001b[38;5;241m=\u001b[39m auc\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Print AUC scores for this combination\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\eniang.eniang\\AppData\\Local\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32md:\\Users\\eniang.eniang\\AppData\\Local\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:635\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m multi_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_class must be in (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movo\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 635\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_multiclass_roc_auc_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    639\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_true)\n",
      "File \u001b[1;32md:\\Users\\eniang.eniang\\AppData\\Local\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:752\u001b[0m, in \u001b[0;36m_multiclass_roc_auc_score\u001b[1;34m(y_true, y_score, labels, multi_class, average, sample_weight)\u001b[0m\n\u001b[0;32m    750\u001b[0m     classes \u001b[38;5;241m=\u001b[39m _unique(y_true)\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(classes) \u001b[38;5;241m!=\u001b[39m y_score\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 752\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    753\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of classes in y_true not equal to the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    754\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_score\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    755\u001b[0m         )\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124movo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Number of classes in y_true not equal to the number of columns in 'y_score'"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE = 16  # Ensure 625 steps for 10000 images\n",
    "EPOCHS = 10\n",
    "\n",
    "def extended_ablation_study(train_dataset, val_dataset, tasks_to_include, input_shape=(224, 224, 3), num_anomaly_types=14):\n",
    "    # Filter dataset labels to match tasks_to_include\n",
    "    def filter_labels(image, labels):\n",
    "        filtered_labels = {task: labels[task] for task in tasks_to_include}\n",
    "        return image, filtered_labels\n",
    "    \n",
    "    # Re-batch to ensure correct batch size\n",
    "    filtered_train_dataset = train_dataset.map(filter_labels, num_parallel_calls=tf.data.AUTOTUNE).unbatch().batch(BATCH_SIZE)\n",
    "    filtered_val_dataset = val_dataset.map(filter_labels, num_parallel_calls=tf.data.AUTOTUNE).unbatch().batch(BATCH_SIZE)\n",
    "    \n",
    "    # Verify filtered dataset\n",
    "    for images, labels in filtered_train_dataset.take(1):\n",
    "        print(f\"Extended ablation train batch: images shape {images.shape}, labels {{ {', '.join(f'{task}: {labels[task].shape}' for task in labels)} }}\")\n",
    "    for images, labels in filtered_val_dataset.take(1):\n",
    "        print(f\"Extended ablation val batch: images shape {images.shape}, labels {{ {', '.join(f'{task}: {labels[task].shape}' for task in labels)} }}\")\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    x = base_model(inputs)\n",
    "    pooled = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    shared_dense = tf.keras.layers.Dense(512, activation='relu')(pooled)\n",
    "    \n",
    "    outputs = {}\n",
    "    loss_dict = {}\n",
    "    metrics_dict = {}\n",
    "    \n",
    "    for task in tasks_to_include:\n",
    "        if task in ['general_anomaly', 'violence', 'property_crime']:\n",
    "            output = tf.keras.layers.Dense(1, activation='sigmoid', name=task)(shared_dense)\n",
    "            loss_dict[task] = tf.keras.losses.BinaryCrossentropy()\n",
    "            metrics_dict[task] = ['accuracy']\n",
    "        elif task == 'anomaly_type':\n",
    "            output = tf.keras.layers.Dense(num_anomaly_types, activation='softmax', name=task)(shared_dense)\n",
    "            loss_dict[task] = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "            metrics_dict[task] = ['accuracy']\n",
    "        outputs[task] = output\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Debug model output shapes\n",
    "    print(\"Model output shapes:\")\n",
    "    for output_name, output_tensor in model.output.items():\n",
    "        print(f\"{output_name}: {output_tensor.shape}\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=loss_dict,\n",
    "        metrics=metrics_dict\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        filtered_train_dataset,\n",
    "        validation_data=filtered_val_dataset,\n",
    "        epochs=5,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Collect predictions and true labels for all validation batches\n",
    "    y_true_dict = {task: [] for task in tasks_to_include}\n",
    "    y_score_dict = {task: [] for task in tasks_to_include}\n",
    "    \n",
    "    for batch_images, batch_labels in filtered_val_dataset:\n",
    "        print(f\"Prediction batch_images shape: {batch_images.shape}\")\n",
    "        val_preds = model.predict(batch_images, verbose=0)\n",
    "        # Handle dict output from model.predict\n",
    "        if isinstance(val_preds, dict):\n",
    "            val_preds = {task: val_preds[task] for task in tasks_to_include}\n",
    "        else:\n",
    "            val_preds = {task: pred for task, pred in zip(tasks_to_include, val_preds)}\n",
    "        \n",
    "        # Store true labels and predictions\n",
    "        for task in tasks_to_include:\n",
    "            y_true_dict[task].append(batch_labels[task].numpy())\n",
    "            y_score_dict[task].append(val_preds[task])\n",
    "    \n",
    "    # Concatenate across batches\n",
    "    auc_scores = {}\n",
    "    for task in tasks_to_include:\n",
    "        y_true = np.concatenate(y_true_dict[task], axis=0)\n",
    "        y_score = np.concatenate(y_score_dict[task], axis=0)\n",
    "        print(f\"{task} - y_true shape: {y_true.shape}, y_score shape: {y_score.shape}\")\n",
    "        \n",
    "        if task in ['general_anomaly', 'violence', 'property_crime']:\n",
    "            # Binary classification AUC\n",
    "            auc = roc_auc_score(y_true.flatten(), y_score.flatten())\n",
    "            auc_scores[task] = auc\n",
    "        elif task == 'anomaly_type':\n",
    "            # Multi-class AUC (one-vs-rest, macro-averaged)\n",
    "            auc = roc_auc_score(y_true, y_score, multi_class='ovr')\n",
    "            auc_scores[task] = auc\n",
    "    \n",
    "    # Print AUC scores for this combination\n",
    "    print(f\"\\nAUC Scores for tasks {tasks_to_include}:\")\n",
    "    for task, auc in auc_scores.items():\n",
    "        print(f\"- {task}: {auc:.4f}\")\n",
    "    \n",
    "    return auc_scores\n",
    "\n",
    "print(\"\\nExtended Ablation Study Results (Individual AUC Scores):\")\n",
    "task_combinations = [\n",
    "    ['general_anomaly'],\n",
    "    ['general_anomaly', 'violence', 'property_crime'],\n",
    "    ['general_anomaly', 'anomaly_type'],\n",
    "    ['general_anomaly', 'violence', 'property_crime', 'anomaly_type']\n",
    "]\n",
    "\n",
    "# Store results for summary\n",
    "all_auc_scores = []\n",
    "\n",
    "for tasks in task_combinations:\n",
    "    auc_scores = extended_ablation_study(train_dataset, val_dataset, tasks)\n",
    "    all_auc_scores.append((tasks, auc_scores))\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\nSummary of AUC Scores Across Task Combinations:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Combination':<40} {'Tasks and AUCs'}\")\n",
    "print(\"-\" * 60)\n",
    "for tasks, auc_scores in all_auc_scores:\n",
    "    task_auc_str = ', '.join(f\"{task}: {auc:.4f}\" for task, auc in auc_scores.items())\n",
    "    print(f\"{str(tasks):<40} {task_auc_str}\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e1b344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
